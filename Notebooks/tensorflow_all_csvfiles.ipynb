{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependencies\n",
    "import glob, os\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training a neural network on Iris dataset using TensorFlow \n",
      "Loading the Iris data to memory...\n",
      "           OBJID   POOLID  ATTRLENGTH      BFSIZE  HDRSIZE\n",
      "0      180959341 -1000000           0   1419264.0      481\n",
      "1      181053640 -1000000           0   2795520.0      491\n",
      "2      181152502 -1000000           0   3388416.0      508\n",
      "3      215387508 -1000000           0   4898816.0      329\n",
      "4      215390564 -1000000           0  11862016.0      354\n",
      "5      215393111 -1000000           0  11862016.0      353\n",
      "6      215395053 -1000000           0  11862016.0      353\n",
      "7      215397191 -1000000           0  11862016.0      353\n",
      "8      215408700 -1000000           0   4381696.0      361\n",
      "9      215422332 -1000000           0   6739968.0      368\n",
      "10     216637962        4           0    697344.0      385\n",
      "11     216714468        4           0    928768.0      479\n",
      "12     218057161       42           0    112640.0      492\n",
      "13     218092913       42           0    252928.0      515\n",
      "14     218161784       42           0    201728.0      530\n",
      "15     218227864       42           0    248832.0      490\n",
      "16     218263386       42           0     98304.0      555\n",
      "17     218993272       42           0   5646336.0      467\n",
      "18     218994099       42           0   2265088.0      517\n",
      "19     218994744       42           0   7068672.0      479\n",
      "20     218996554       42           0   7136256.0      551\n",
      "21     218997860       42           0   5210112.0      521\n",
      "22     218998080       42           0   5210112.0      520\n",
      "23     219000195       42           0   4373504.0      519\n",
      "24     219000290       42           0   4373504.0      519\n",
      "25     219001687       42           0   3473408.0      564\n",
      "26     219007023       42           0   1785856.0      568\n",
      "27     219010990       42           0   5989376.0      576\n",
      "28     219011114       42           0   5989376.0      575\n",
      "29     219357535       42           0  18344960.0      361\n",
      "...          ...      ...         ...         ...      ...\n",
      "10979  218996668       42           0   7136256.0      549\n",
      "10980  218996704       42           0   7136256.0      550\n",
      "10981  219005056       42           0   2091008.0      493\n",
      "10982  219007177       42           0   1785856.0      566\n",
      "10983  219366422       42           0    402432.0      430\n",
      "10984  219387156       42           0    334848.0      453\n",
      "10985  219416311       42           0    132096.0      468\n",
      "10986  219423464       42           0    614400.0      471\n",
      "10987  219433190       42           0    120832.0      435\n",
      "10988  219438367       42           0    343040.0      488\n",
      "10989  219453237       42           0   6804480.0      381\n",
      "10990  219470482       42           0    607232.0      529\n",
      "10991  219510450       42           0    220160.0      410\n",
      "10992  219562488       42           0    413696.0      481\n",
      "10993  219562919       42           0   2423808.0      441\n",
      "10994  219579454       42           0   6191104.0      422\n",
      "10995  219579887       42           0   1312768.0      412\n",
      "10996  219580988       42           0  10669056.0      416\n",
      "10997  219585878       42           0   3639296.0      387\n",
      "10998  219588720       42           0   4915200.0      413\n",
      "10999  219589045       42           0  10184704.0      401\n",
      "11000  219589151       42           0  10184704.0      402\n",
      "11001  219590406       42           0   2869248.0      357\n",
      "11002  220906751       42           0    601088.0      344\n",
      "11003  220906751       -9           0    601088.0      344\n",
      "11004  220938962       42           0     31744.0      347\n",
      "11005  220938962       -9           0     31744.0      347\n",
      "11006  220943829       42           0     67584.0      347\n",
      "11007  220943829       -9           0     67584.0      347\n",
      "11008  238236329        4           0     58368.0      454\n",
      "\n",
      "[11009 rows x 5 columns]\n",
      "Finish loading\n"
     ]
    }
   ],
   "source": [
    "path = '/Users/zhaoluyang/Downloads/Senior-Capstone-2018-2019-master/Notebooks/TestData/merge_data/4_features'                     \n",
    "all_files = glob.glob(os.path.join(path, \"*.csv\"))     # advisable to use os.path.join as this makes concatenation OS independent\n",
    "\n",
    "df_from_each_file = (pd.read_csv(f) for f in all_files)\n",
    "concatenated_df   = pd.concat(df_from_each_file, ignore_index=True)\n",
    "\n",
    "print(\"\\nTraining a neural network on Iris dataset using TensorFlow \")\n",
    "print(\"Loading the Iris data to memory...\")\n",
    "# Loading the dataset\n",
    "# dataset = pd.read_csv('ourdata.csv')\n",
    "print(concatenated_df)\n",
    "print(\"Finish loading\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           OBJID  ATTRLENGTH      BFSIZE  HDRSIZE  POOLID_-1000000  POOLID_-9  \\\n",
      "0      180959341           0   1419264.0      481                1          0   \n",
      "1      181053640           0   2795520.0      491                1          0   \n",
      "2      181152502           0   3388416.0      508                1          0   \n",
      "3      215387508           0   4898816.0      329                1          0   \n",
      "4      215390564           0  11862016.0      354                1          0   \n",
      "5      215393111           0  11862016.0      353                1          0   \n",
      "6      215395053           0  11862016.0      353                1          0   \n",
      "7      215397191           0  11862016.0      353                1          0   \n",
      "8      215408700           0   4381696.0      361                1          0   \n",
      "9      215422332           0   6739968.0      368                1          0   \n",
      "10     216637962           0    697344.0      385                0          0   \n",
      "11     216714468           0    928768.0      479                0          0   \n",
      "12     218057161           0    112640.0      492                0          0   \n",
      "13     218092913           0    252928.0      515                0          0   \n",
      "14     218161784           0    201728.0      530                0          0   \n",
      "15     218227864           0    248832.0      490                0          0   \n",
      "16     218263386           0     98304.0      555                0          0   \n",
      "17     218993272           0   5646336.0      467                0          0   \n",
      "18     218994099           0   2265088.0      517                0          0   \n",
      "19     218994744           0   7068672.0      479                0          0   \n",
      "20     218996554           0   7136256.0      551                0          0   \n",
      "21     218997860           0   5210112.0      521                0          0   \n",
      "22     218998080           0   5210112.0      520                0          0   \n",
      "23     219000195           0   4373504.0      519                0          0   \n",
      "24     219000290           0   4373504.0      519                0          0   \n",
      "25     219001687           0   3473408.0      564                0          0   \n",
      "26     219007023           0   1785856.0      568                0          0   \n",
      "27     219010990           0   5989376.0      576                0          0   \n",
      "28     219011114           0   5989376.0      575                0          0   \n",
      "29     219357535           0  18344960.0      361                0          0   \n",
      "...          ...         ...         ...      ...              ...        ...   \n",
      "10979  218996668           0   7136256.0      549                0          0   \n",
      "10980  218996704           0   7136256.0      550                0          0   \n",
      "10981  219005056           0   2091008.0      493                0          0   \n",
      "10982  219007177           0   1785856.0      566                0          0   \n",
      "10983  219366422           0    402432.0      430                0          0   \n",
      "10984  219387156           0    334848.0      453                0          0   \n",
      "10985  219416311           0    132096.0      468                0          0   \n",
      "10986  219423464           0    614400.0      471                0          0   \n",
      "10987  219433190           0    120832.0      435                0          0   \n",
      "10988  219438367           0    343040.0      488                0          0   \n",
      "10989  219453237           0   6804480.0      381                0          0   \n",
      "10990  219470482           0    607232.0      529                0          0   \n",
      "10991  219510450           0    220160.0      410                0          0   \n",
      "10992  219562488           0    413696.0      481                0          0   \n",
      "10993  219562919           0   2423808.0      441                0          0   \n",
      "10994  219579454           0   6191104.0      422                0          0   \n",
      "10995  219579887           0   1312768.0      412                0          0   \n",
      "10996  219580988           0  10669056.0      416                0          0   \n",
      "10997  219585878           0   3639296.0      387                0          0   \n",
      "10998  219588720           0   4915200.0      413                0          0   \n",
      "10999  219589045           0  10184704.0      401                0          0   \n",
      "11000  219589151           0  10184704.0      402                0          0   \n",
      "11001  219590406           0   2869248.0      357                0          0   \n",
      "11002  220906751           0    601088.0      344                0          0   \n",
      "11003  220906751           0    601088.0      344                0          1   \n",
      "11004  220938962           0     31744.0      347                0          0   \n",
      "11005  220938962           0     31744.0      347                0          1   \n",
      "11006  220943829           0     67584.0      347                0          0   \n",
      "11007  220943829           0     67584.0      347                0          1   \n",
      "11008  238236329           0     58368.0      454                0          0   \n",
      "\n",
      "       POOLID_-1  POOLID_4  POOLID_6  POOLID_42  POOLID_72  POOLID_82  \n",
      "0              0         0         0          0          0          0  \n",
      "1              0         0         0          0          0          0  \n",
      "2              0         0         0          0          0          0  \n",
      "3              0         0         0          0          0          0  \n",
      "4              0         0         0          0          0          0  \n",
      "5              0         0         0          0          0          0  \n",
      "6              0         0         0          0          0          0  \n",
      "7              0         0         0          0          0          0  \n",
      "8              0         0         0          0          0          0  \n",
      "9              0         0         0          0          0          0  \n",
      "10             0         1         0          0          0          0  \n",
      "11             0         1         0          0          0          0  \n",
      "12             0         0         0          1          0          0  \n",
      "13             0         0         0          1          0          0  \n",
      "14             0         0         0          1          0          0  \n",
      "15             0         0         0          1          0          0  \n",
      "16             0         0         0          1          0          0  \n",
      "17             0         0         0          1          0          0  \n",
      "18             0         0         0          1          0          0  \n",
      "19             0         0         0          1          0          0  \n",
      "20             0         0         0          1          0          0  \n",
      "21             0         0         0          1          0          0  \n",
      "22             0         0         0          1          0          0  \n",
      "23             0         0         0          1          0          0  \n",
      "24             0         0         0          1          0          0  \n",
      "25             0         0         0          1          0          0  \n",
      "26             0         0         0          1          0          0  \n",
      "27             0         0         0          1          0          0  \n",
      "28             0         0         0          1          0          0  \n",
      "29             0         0         0          1          0          0  \n",
      "...          ...       ...       ...        ...        ...        ...  \n",
      "10979          0         0         0          1          0          0  \n",
      "10980          0         0         0          1          0          0  \n",
      "10981          0         0         0          1          0          0  \n",
      "10982          0         0         0          1          0          0  \n",
      "10983          0         0         0          1          0          0  \n",
      "10984          0         0         0          1          0          0  \n",
      "10985          0         0         0          1          0          0  \n",
      "10986          0         0         0          1          0          0  \n",
      "10987          0         0         0          1          0          0  \n",
      "10988          0         0         0          1          0          0  \n",
      "10989          0         0         0          1          0          0  \n",
      "10990          0         0         0          1          0          0  \n",
      "10991          0         0         0          1          0          0  \n",
      "10992          0         0         0          1          0          0  \n",
      "10993          0         0         0          1          0          0  \n",
      "10994          0         0         0          1          0          0  \n",
      "10995          0         0         0          1          0          0  \n",
      "10996          0         0         0          1          0          0  \n",
      "10997          0         0         0          1          0          0  \n",
      "10998          0         0         0          1          0          0  \n",
      "10999          0         0         0          1          0          0  \n",
      "11000          0         0         0          1          0          0  \n",
      "11001          0         0         0          1          0          0  \n",
      "11002          0         0         0          1          0          0  \n",
      "11003          0         0         0          0          0          0  \n",
      "11004          0         0         0          1          0          0  \n",
      "11005          0         0         0          0          0          0  \n",
      "11006          0         0         0          1          0          0  \n",
      "11007          0         0         0          0          0          0  \n",
      "11008          0         1         0          0          0          0  \n",
      "\n",
      "[11009 rows x 12 columns]\n"
     ]
    }
   ],
   "source": [
    "# One-hot encoding for the categories\n",
    "norma = pd.get_dummies(concatenated_df, columns=['POOLID']) \n",
    "values = list(concatenated_dataset.columns.values)\n",
    "print(concatenated_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          OBJID  ATTRLENGTH    BFSIZE   HDRSIZE\n",
      "0      0.093968         0.0  0.000788  0.120415\n",
      "1      0.095454         0.0  0.001552  0.127336\n",
      "2      0.097012         0.0  0.001882  0.139100\n",
      "3      0.636472         0.0  0.002721  0.015225\n",
      "4      0.636520         0.0  0.006589  0.032526\n",
      "5      0.636561         0.0  0.006589  0.031834\n",
      "6      0.636591         0.0  0.006589  0.031834\n",
      "7      0.636625         0.0  0.006589  0.031834\n",
      "8      0.636806         0.0  0.002434  0.037370\n",
      "9      0.637021         0.0  0.003744  0.042215\n",
      "10     0.656176         0.0  0.000387  0.053979\n",
      "11     0.657382         0.0  0.000515  0.119031\n",
      "12     0.678539         0.0  0.000062  0.128028\n",
      "13     0.679103         0.0  0.000140  0.143945\n",
      "14     0.680188         0.0  0.000111  0.154325\n",
      "15     0.681229         0.0  0.000138  0.126644\n",
      "16     0.681789         0.0  0.000054  0.171626\n",
      "17     0.693290         0.0  0.003136  0.110727\n",
      "18     0.693303         0.0  0.001258  0.145329\n",
      "19     0.693313         0.0  0.003926  0.119031\n",
      "20     0.693342         0.0  0.003964  0.168858\n",
      "21     0.693363         0.0  0.002894  0.148097\n",
      "22     0.693366         0.0  0.002894  0.147405\n",
      "23     0.693399         0.0  0.002429  0.146713\n",
      "24     0.693401         0.0  0.002429  0.146713\n",
      "25     0.693423         0.0  0.001929  0.177855\n",
      "26     0.693507         0.0  0.000992  0.180623\n",
      "27     0.693569         0.0  0.003327  0.186159\n",
      "28     0.693571         0.0  0.003327  0.185467\n",
      "29     0.699030         0.0  0.010191  0.037370\n",
      "...         ...         ...       ...       ...\n",
      "10979  0.693344         0.0  0.003964  0.167474\n",
      "10980  0.693344         0.0  0.003964  0.168166\n",
      "10981  0.693476         0.0  0.001161  0.128720\n",
      "10982  0.693509         0.0  0.000992  0.179239\n",
      "10983  0.699170         0.0  0.000223  0.085121\n",
      "10984  0.699497         0.0  0.000185  0.101038\n",
      "10985  0.699956         0.0  0.000073  0.111419\n",
      "10986  0.700069         0.0  0.000341  0.113495\n",
      "10987  0.700222         0.0  0.000067  0.088581\n",
      "10988  0.700304         0.0  0.000190  0.125260\n",
      "10989  0.700538         0.0  0.003780  0.051211\n",
      "10990  0.700810         0.0  0.000337  0.153633\n",
      "10991  0.701440         0.0  0.000122  0.071280\n",
      "10992  0.702260         0.0  0.000229  0.120415\n",
      "10993  0.702267         0.0  0.001346  0.092734\n",
      "10994  0.702527         0.0  0.003439  0.079585\n",
      "10995  0.702534         0.0  0.000729  0.072664\n",
      "10996  0.702551         0.0  0.005926  0.075433\n",
      "10997  0.702628         0.0  0.002021  0.055363\n",
      "10998  0.702673         0.0  0.002730  0.073356\n",
      "10999  0.702678         0.0  0.005657  0.065052\n",
      "11000  0.702680         0.0  0.005657  0.065744\n",
      "11001  0.702700         0.0  0.001593  0.034602\n",
      "11002  0.723442         0.0  0.000333  0.025606\n",
      "11003  0.723442         0.0  0.000333  0.025606\n",
      "11004  0.723950         0.0  0.000017  0.027682\n",
      "11005  0.723950         0.0  0.000017  0.027682\n",
      "11006  0.724026         0.0  0.000037  0.027682\n",
      "11007  0.724026         0.0  0.000037  0.027682\n",
      "11008  0.996514         0.0  0.000032  0.101730\n",
      "\n",
      "[11009 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "#Normalization\n",
    "X_train = concatenated_dataset[values[:4]]\n",
    "X_train = ((X_train - X_train.min()) / (X_train.max() - X_train.min())).fillna(0)\n",
    "print(X_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[7.0088e-01 0.0000e+00 1.1946e-04 1.0311e-01]\n",
      " [6.8010e-01 0.0000e+00 1.7919e-04 2.0000e-01]\n",
      " [7.0169e-01 0.0000e+00 2.3039e-04 8.1661e-02]\n",
      " ...\n",
      " [9.5697e-02 0.0000e+00 1.1571e-03 1.3426e-01]\n",
      " [6.9335e-01 0.0000e+00 1.8960e-03 1.4533e-01]\n",
      " [7.0257e-01 0.0000e+00 2.4854e-03 8.0969e-02]]\n"
     ]
    }
   ],
   "source": [
    "#preprocess the data \n",
    "features = np.array(X_train, dtype='float32')\n",
    "target = np.array(concatenated_dataset[values[4:]], dtype='float32')\n",
    "\n",
    "# Shuffle Data\n",
    "indices = np.random.choice(len(features), len(features), replace=False)\n",
    "X_values = features[indices]\n",
    "y_values = target[indices]\n",
    "\n",
    "# Creating a Train and a Test Dataset\n",
    "test_size = 1000\n",
    "X_test = X_values[-test_size:]\n",
    "X_train = X_values[:-test_size]\n",
    "y_test = y_values[-test_size:]\n",
    "y_train = y_values[:-test_size]\n",
    "\n",
    "# print(X_train)\n",
    "print(X_test)\n",
    "# print(y_test)\n",
    "# print(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A neural Network which contains 3 layers with 4, 10, 8 nodes repectively was created!\n"
     ]
    }
   ],
   "source": [
    "# define a neural network\n",
    "\n",
    "# Initialize placeholders\n",
    "X_data = tf.placeholder(shape=[None, 4], dtype=tf.float32)\n",
    "y_target = tf.placeholder(shape=[None, 8], dtype=tf.float32)\n",
    "\n",
    "#create seed for random_normal()\n",
    "seed = 1234\n",
    "np.random.seed(seed)\n",
    "tf.set_random_seed(seed)\n",
    "\n",
    "hidden_layer_nodes = 10\n",
    "# We create a neural Network which contains 3 layers with 4, 8, 3 nodes repectively\n",
    "w1 = tf.Variable(tf.random_normal(shape=[4,hidden_layer_nodes])) # Weight of the input layer\n",
    "b1 = tf.Variable(tf.random_normal(shape=[hidden_layer_nodes]))   # Bias of the input layer\n",
    "w2 = tf.Variable(tf.random_normal(shape=[hidden_layer_nodes,8])) # Weight of the hidden layer\n",
    "b2 = tf.Variable(tf.random_normal(shape=[8]))                    # Bias of the hidden layer\n",
    "hidden_output = tf.nn.relu(tf.add(tf.matmul(X_data, w1), b1))\n",
    "final_output = tf.nn.softmax(tf.add(tf.matmul(hidden_output, w2), b2))\n",
    "\n",
    "# Loss Function\n",
    "loss = tf.reduce_mean(-tf.reduce_sum(y_target * tf.log(final_output), axis=0))\n",
    "\n",
    "# Optimizer\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.001).minimize(loss)\n",
    "\n",
    "print(\"A neural Network which contains 3 layers with 4, 10, 8 nodes repectively was created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the model...\n",
      "Epoch 100 | Loss: 973.0404\n",
      "Epoch 200 | Loss: 841.7213\n",
      "Epoch 300 | Loss: 755.81616\n",
      "Epoch 400 | Loss: 659.33374\n",
      "Epoch 500 | Loss: 727.3075\n",
      "Epoch 600 | Loss: 814.311\n",
      "Epoch 700 | Loss: 711.9629\n",
      "Epoch 800 | Loss: 676.01624\n",
      "Epoch 900 | Loss: 640.03534\n",
      "Epoch 1000 | Loss: 509.75586\n",
      "Training finished\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Training the model...')\n",
    "\n",
    "# Interval / Epochs\n",
    "interval = 100\n",
    "epoch = 1000\n",
    "\n",
    "# Initialize variables\n",
    "sess = tf.Session()\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "\n",
    "# Training the model...\n",
    "for i in range(1, (epoch + 1)):\n",
    "    sess.run(optimizer, feed_dict={X_data: X_train, y_target: y_train})\n",
    "    if i % interval == 0:\n",
    "        print('Epoch', i, '|', 'Loss:', sess.run(loss, feed_dict={X_data: X_train, y_target: y_train}))\n",
    "\n",
    "print(\"Training finished\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of the model is: 0.902\n"
     ]
    }
   ],
   "source": [
    "# get the accuracy of the model\n",
    "correct_prediction = tf.equal(tf.argmax(final_output, 1), tf.argmax(y_target,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "print(\"The accuracy of the model is:\", sess.run(accuracy, feed_dict={X_data: X_test, y_target: y_test}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using model to predict pool id for features:  [[0.6934 0.     0.0029 0.1481]]\n",
      "\n",
      "Predicted softmax vector is:  [[6.6219e-03 2.6045e-04 3.2724e-06 5.8033e-03 7.3191e-04 9.8653e-01\n",
      "  8.8598e-06 4.4372e-05]]\n",
      "\n",
      "Predicted pool id is:  POOLID_6\n"
     ]
    }
   ],
   "source": [
    "# Prediction\n",
    "np.set_printoptions(precision=4)\n",
    "unknown = np.array([[0.693363, 0.0, 0.002894, 0.148097]], dtype=np.float32)\n",
    "predicted = sess.run(final_output, feed_dict={X_data: unknown})\n",
    "# model.predict(unknown)\n",
    "print(\"Using model to predict pool id for features: \", unknown)\n",
    "print(\"\\nPredicted softmax vector is: \",predicted)\n",
    "Class_dict={'POOLID_-1000000': 0, 'POOLID_-9': 1, 'POOLID_-1': 2, 'POOLID_4': 3, 'POOLID_-1': 4, 'POOLID_6': 5, 'POOLID_42': 6, 'POOLID_72': 7, 'POOLID_82': 8 }\n",
    "pool_dict = {v:k for k,v in Class_dict.items()}\n",
    "print(\"\\nPredicted pool id is: \", pool_dict[np.argmax(predicted)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
