{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PreProcessing Parquet Files for use with Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import SQLContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initiates a SparkContext which is necessary for accessing data in Spark\n",
    "sc = SparkContext()\n",
    "sqlContext = SQLContext(sc)\n",
    "# change to match your environment\n",
    "output_dir = \"/home/austin/Documents/Senior-Capstone-2018-2019/data/TestData/merged_data/SparkOutput\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the cell below, uncomment the code to get a list of all the StructFields (column names). This is handy for figuring out which features to choose as they give file type for each column as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# sqlContext.read.parquet(\"/home/austin/Documents/Senior-Capstone-2018-2019/data/TestData/merged_data/part.*.parquet\").schema.fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('POOLID', 'bigint'),\n",
       " ('SIZE', 'bigint'),\n",
       " ('OFFSET', 'bigint'),\n",
       " ('HDRSIZE', 'bigint')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = sqlContext.read.parquet(\"/home/austin/Documents/Senior-Capstone-2018-2019/data/TestData/merged_data/part.*.parquet\").select(\"POOLID\", \"SIZE\", \"OFFSET\", \"HDRSIZE\")\n",
    "df.dtypes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below takes the four columns that we want and writes them to a folder of our choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.parquet(output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below then goes and reads the parquet files generated by the cell above and prints out all the new column names and types to make sure we outputted what we wanted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[StructField(POOLID,LongType,true),\n",
       " StructField(SIZE,LongType,true),\n",
       " StructField(OFFSET,LongType,true),\n",
       " StructField(HDRSIZE,LongType,true)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sqlContext.read.parquet(\"/home/austin/Documents/Senior-Capstone-2018-2019/data/TestData/merged_data/SparkOutput/part-*.parquet\").schema.fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
