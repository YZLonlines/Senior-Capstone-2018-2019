{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change to be reflective of your environment\n",
    "data_dir = '/home/cole/Workspace/School/Capstone/data/first_data_set/TestData/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "size = os.path.getsize(data_dir+\"/BACKUP_OBJECTS.csv\")\n",
    "end = 50 # how many gigs to scale backup objects\n",
    "upscale = int((end * 1073741824) / size)\n",
    "upscale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initiates a SparkContext which is necessary for accessing data in Spark\n",
    "sc = SparkContext()\n",
    "sqlContext = SQLContext(sc)\n",
    "# change to match your environment\n",
    "output_dir = data_dir + \"/merge_data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "SDRO = sqlContext.read.format('com.databricks.spark.csv').option(\"header\", \"true\").load([data_dir + '/SD_RECON_ORDER.csv'])\n",
    "SS_POOLS = sqlContext.read.format('com.databricks.spark.csv').option(\"header\", \"true\").load([data_dir + '/SS_POOLS.csv'])\n",
    "AFBF = sqlContext.read.format('com.databricks.spark.csv').option(\"header\", \"true\").load([data_dir+\"/AF_BITFILES.csv\"])\n",
    "BACKUP_OBJECTS = sqlContext.read.format('com.databricks.spark.csv').option(\"header\", \"true\").load([data_dir+\"/BACKUP_OBJECTS.csv\"])\n",
    "ls = [SDRO, SS_POOLS, AFBF, BACKUP_OBJECTS]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53161928\n",
      "744266992\n",
      "113\n",
      "1582\n",
      "18048\n",
      "252672\n",
      "5823871\n",
      "81534194\n"
     ]
    }
   ],
   "source": [
    "# upscale step\n",
    "for i in range(len(ls)):\n",
    "    csv = ls[i]\n",
    "    concat = csv\n",
    "    print(concat.count())\n",
    "    for j in range(upscale):\n",
    "        concat = concat.union(csv)\n",
    "    ls[i] = concat\n",
    "    print(concat.count())\n",
    "    \n",
    "    \n",
    "SDRO = ls[0]\n",
    "SS_POOLS = ls[1]\n",
    "AFBF = ls[2]\n",
    "BACKUP_OBJECTS = ls[3]\n",
    "\n",
    "# # save new ones before join then reload\n",
    "# SDRO.write.options(header='true').format('com.databricks.spark.csv').save(data_dir + \"/upscale_\" + str(end) + \"_SDRO\")\n",
    "# SS_POOLS.write.options(header='true').format('com.databricks.spark.csv').save(data_dir + \"/upscale_\" + str(end) + \"_SS_POOLS\")\n",
    "# AFBF.write.options(header='true').format('com.databricks.spark.csv').save(data_dir + \"/upscale_\" + str(end) + \"_AFBF\")\n",
    "# BACKUP_OBJECTS.write.options(header='true').format('com.databricks.spark.csv').save(data_dir + \"/upscale_\" + str(end) + \"_BACKUP_OBJECTS\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SDRO = None\n",
    "# SS_POOLS = None\n",
    "# AFBF = None\n",
    "# BACKUP_OBJECTS = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SDRO = sqlContext.read.format('com.databricks.spark.csv').option(\"header\", \"true\").load(data_dir + \"/upscale_\" + str(end) + \"_SDRO/*.csv\")\n",
    "# SS_POOLS = sqlContext.read.format('com.databricks.spark.csv').option(\"header\", \"true\").load(data_dir + \"/upscale_\" + str(end) + \"_SS_POOLS/*.csv\")\n",
    "# AFBF = sqlContext.read.format('com.databricks.spark.csv').option(\"header\", \"true\").load(data_dir + \"/upscale_\" + str(end) + \"_AFBF/*.csv\")\n",
    "# BACKUP_OBJECTS = sqlContext.read.format('com.databricks.spark.csv').option(\"header\", \"true\").load(data_dir + \"/upscale_\" + str(end) + \"_BACKUP_OBJECTS/*.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_outer_join = BACKUP_OBJECTS.join(AFBF, BACKUP_OBJECTS.OBJID == AFBF.BFID,how='left') # Could also use 'full_outer'\n",
    "full_outer_join = full_outer_join.join(SDRO, ['OBJID'],how='left') # Could also use 'full_outer'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "SDRO = None\n",
    "SS_POOLS = None\n",
    "AFBF = None\n",
    "BACKUP_OBJECTS = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = full_outer_join\n",
    "df = df.filter(df.POOLID. isNotNull())\n",
    "df = df.withColumn(\"POOLID\", df[\"POOLID\"].cast(\"int\"))\n",
    "df = df.filter(df.ATTRLENGTH. isNotNull())\n",
    "df = df.withColumn(\"ATTRLENGTH\", df[\"ATTRLENGTH\"].cast(\"float\"))\n",
    "df = df.filter(df.BFSIZE. isNotNull())\n",
    "df = df.withColumn(\"BFSIZE\", df[\"BFSIZE\"].cast(\"float\"))\n",
    "df = df.filter(df.HDRSIZE. isNotNull())\n",
    "df = df.withColumn(\"HDRSIZE\", df[\"HDRSIZE\"].cast(\"float\"))\n",
    "df = df.filter(df.OBJID. isNotNull())\n",
    "df = df.withColumn(\"OBJID\", df[\"OBJID\"].cast(\"float\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when   \n",
    "\n",
    "df = df.withColumn('POOLID', when(df['POOLID'] == -1000000, 0).otherwise(df['POOLID']))\n",
    "df = df.withColumn('POOLID', when(df['POOLID'] == -9, 1).otherwise(df['POOLID']))\n",
    "df = df.withColumn('POOLID', when(df['POOLID'] == 4, 2).otherwise(df['POOLID']))\n",
    "df = df.withColumn('POOLID', when(df['POOLID'] == 6, 3).otherwise(df['POOLID']))\n",
    "df = df.withColumn('POOLID', when(df['POOLID'] == 42, 4).otherwise(df['POOLID']))\n",
    "df = df.withColumn('POOLID', when(df['POOLID'] == 72, 5).otherwise(df['POOLID']))\n",
    "df = df.withColumn('POOLID', when(df['POOLID'] == 82, 6).otherwise(df['POOLID']))\n",
    "df = df.withColumn('POOLID', when(df['POOLID'] == -1, 7).otherwise(df['POOLID']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.filter(df.POOLID. isNotNull())\n",
    "df = df.withColumn(\"POOLID\", df[\"POOLID\"].cast(\"int\"))\n",
    "df = df.filter(df.ATTRLENGTH. isNotNull())\n",
    "df = df.withColumn(\"ATTRLENGTH\", df[\"ATTRLENGTH\"].cast(\"float\"))\n",
    "df = df.filter(df.BFSIZE. isNotNull())\n",
    "df = df.withColumn(\"BFSIZE\", df[\"BFSIZE\"].cast(\"float\"))\n",
    "df = df.filter(df.HDRSIZE. isNotNull())\n",
    "df = df.withColumn(\"HDRSIZE\", df[\"HDRSIZE\"].cast(\"float\"))\n",
    "df = df.filter(df.OBJID. isNotNull())\n",
    "df = df.withColumn(\"OBJID\", df[\"OBJID\"].cast(\"float\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select(\"OBJID\", \"ATTRLENGTH\", \"BFSIZE\", \"HDRSIZE\", \"POOLID\").write.options(header='true').format('com.databricks.spark.csv').save(data_dir + \"/merge_data/upscale\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependencies\n",
    "import glob, os\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training a neural network on Spectrum Protect data using TensorFlow \n",
      "Loading the Spectrum Protect data to memory...\n",
      "               OBJID  ATTRLENGTH     BFSIZE  HDRSIZE  POOLID\n",
      "0        180976816.0         0.0  2526208.0    466.0       0\n",
      "1        180976816.0         0.0  2526208.0    466.0       0\n",
      "2        180976816.0         0.0  2526208.0    466.0       0\n",
      "3        180976816.0         0.0  2526208.0    466.0       0\n",
      "4        180976816.0         0.0  2526208.0    466.0       0\n",
      "5        180976816.0         0.0  2526208.0    466.0       0\n",
      "6        180976816.0         0.0  2526208.0    466.0       0\n",
      "7        180976816.0         0.0  2526208.0    466.0       0\n",
      "8        180976816.0         0.0  2526208.0    466.0       0\n",
      "9        180976816.0         0.0  2526208.0    466.0       0\n",
      "10       180976816.0         0.0  2526208.0    466.0       0\n",
      "11       180976816.0         0.0  2526208.0    466.0       0\n",
      "12       180976816.0         0.0  2526208.0    466.0       0\n",
      "13       180976816.0         0.0  2526208.0    466.0       0\n",
      "14       180976816.0         0.0  2526208.0    466.0       0\n",
      "15       180976816.0         0.0  2526208.0    466.0       0\n",
      "16       180976816.0         0.0  2526208.0    466.0       0\n",
      "17       180976816.0         0.0  2526208.0    466.0       0\n",
      "18       180976816.0         0.0  2526208.0    466.0       0\n",
      "19       180976816.0         0.0  2526208.0    466.0       0\n",
      "20       180976816.0         0.0  2526208.0    466.0       0\n",
      "21       180976816.0         0.0  2526208.0    466.0       0\n",
      "22       180976816.0         0.0  2526208.0    466.0       0\n",
      "23       180976816.0         0.0  2526208.0    466.0       0\n",
      "24       180976816.0         0.0  2526208.0    466.0       0\n",
      "25       180976816.0         0.0  2526208.0    466.0       0\n",
      "26       180976816.0         0.0  2526208.0    466.0       0\n",
      "27       180976816.0         0.0  2526208.0    466.0       0\n",
      "28       180976816.0         0.0  2526208.0    466.0       0\n",
      "29       180976816.0         0.0  2526208.0    466.0       0\n",
      "...              ...         ...        ...      ...     ...\n",
      "2157734  238429776.0         0.0  2269184.0    454.0       2\n",
      "2157735  238429776.0         0.0  2269184.0    454.0       2\n",
      "2157736  238429776.0         0.0  2269184.0    454.0       2\n",
      "2157737  238429776.0         0.0  2269184.0    454.0       2\n",
      "2157738  238429776.0         0.0  2269184.0    454.0       2\n",
      "2157739  238429776.0         0.0  2269184.0    454.0       2\n",
      "2157740  238429776.0         0.0  2269184.0    454.0       2\n",
      "2157741  238429776.0         0.0  2269184.0    454.0       2\n",
      "2157742  238429776.0         0.0  2269184.0    454.0       2\n",
      "2157743  238429776.0         0.0  2269184.0    454.0       2\n",
      "2157744  238429776.0         0.0  2269184.0    454.0       2\n",
      "2157745  238429776.0         0.0  2269184.0    454.0       2\n",
      "2157746  238429776.0         0.0  2269184.0    454.0       2\n",
      "2157747  238429776.0         0.0  2269184.0    454.0       2\n",
      "2157748  238429776.0         0.0  2269184.0    454.0       2\n",
      "2157749  238429776.0         0.0  2269184.0    454.0       2\n",
      "2157750  238429776.0         0.0  2269184.0    454.0       2\n",
      "2157751  238429776.0         0.0  2269184.0    454.0       2\n",
      "2157752  238429776.0         0.0  2269184.0    454.0       2\n",
      "2157753  238429776.0         0.0  2269184.0    454.0       2\n",
      "2157754  238429776.0         0.0  2269184.0    454.0       2\n",
      "2157755  238429776.0         0.0  2269184.0    454.0       2\n",
      "2157756  238429776.0         0.0  2269184.0    454.0       2\n",
      "2157757  238429776.0         0.0  2269184.0    454.0       2\n",
      "2157758  238429776.0         0.0  2269184.0    454.0       2\n",
      "2157759  238429776.0         0.0  2269184.0    454.0       2\n",
      "2157760  238429776.0         0.0  2269184.0    454.0       2\n",
      "2157761  238429776.0         0.0  2269184.0    454.0       2\n",
      "2157762  238429776.0         0.0  2269184.0    454.0       2\n",
      "2157763  238429776.0         0.0  2269184.0    454.0       2\n",
      "\n",
      "[2157764 rows x 5 columns]\n",
      "Finish loading\n"
     ]
    }
   ],
   "source": [
    "path = '/Users/zhaoluyang/Downloads/Senior-Capstone-2018-2019-master/Notebooks/TestData/merge_data/4_features'                     \n",
    "path = data_dir + \"/merge_data/upscale\"\n",
    "all_files = glob.glob(os.path.join(path, \"*.csv\"))     # advisable to use os.path.join as this makes concatenation OS independent\n",
    "\n",
    "df_from_each_file = (pd.read_csv(f) for f in all_files)\n",
    "concatenated_dataset   = pd.concat(df_from_each_file, ignore_index=True)\n",
    "\n",
    "print(\"\\nTraining a neural network on Spectrum Protect data using TensorFlow \")\n",
    "print(\"Loading the Spectrum Protect data to memory...\")\n",
    "# Loading the dataset\n",
    "# dataset = pd.read_csv('ourdata.csv')\n",
    "print(concatenated_dataset)\n",
    "print(\"Finish loading\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               OBJID  ATTRLENGTH     BFSIZE  HDRSIZE  POOLID_0  POOLID_1  \\\n",
      "0        180976816.0         0.0  2526208.0    466.0         1         0   \n",
      "1        180976816.0         0.0  2526208.0    466.0         1         0   \n",
      "2        180976816.0         0.0  2526208.0    466.0         1         0   \n",
      "3        180976816.0         0.0  2526208.0    466.0         1         0   \n",
      "4        180976816.0         0.0  2526208.0    466.0         1         0   \n",
      "5        180976816.0         0.0  2526208.0    466.0         1         0   \n",
      "6        180976816.0         0.0  2526208.0    466.0         1         0   \n",
      "7        180976816.0         0.0  2526208.0    466.0         1         0   \n",
      "8        180976816.0         0.0  2526208.0    466.0         1         0   \n",
      "9        180976816.0         0.0  2526208.0    466.0         1         0   \n",
      "10       180976816.0         0.0  2526208.0    466.0         1         0   \n",
      "11       180976816.0         0.0  2526208.0    466.0         1         0   \n",
      "12       180976816.0         0.0  2526208.0    466.0         1         0   \n",
      "13       180976816.0         0.0  2526208.0    466.0         1         0   \n",
      "14       180976816.0         0.0  2526208.0    466.0         1         0   \n",
      "15       180976816.0         0.0  2526208.0    466.0         1         0   \n",
      "16       180976816.0         0.0  2526208.0    466.0         1         0   \n",
      "17       180976816.0         0.0  2526208.0    466.0         1         0   \n",
      "18       180976816.0         0.0  2526208.0    466.0         1         0   \n",
      "19       180976816.0         0.0  2526208.0    466.0         1         0   \n",
      "20       180976816.0         0.0  2526208.0    466.0         1         0   \n",
      "21       180976816.0         0.0  2526208.0    466.0         1         0   \n",
      "22       180976816.0         0.0  2526208.0    466.0         1         0   \n",
      "23       180976816.0         0.0  2526208.0    466.0         1         0   \n",
      "24       180976816.0         0.0  2526208.0    466.0         1         0   \n",
      "25       180976816.0         0.0  2526208.0    466.0         1         0   \n",
      "26       180976816.0         0.0  2526208.0    466.0         1         0   \n",
      "27       180976816.0         0.0  2526208.0    466.0         1         0   \n",
      "28       180976816.0         0.0  2526208.0    466.0         1         0   \n",
      "29       180976816.0         0.0  2526208.0    466.0         1         0   \n",
      "...              ...         ...        ...      ...       ...       ...   \n",
      "2157734  238429776.0         0.0  2269184.0    454.0         0         0   \n",
      "2157735  238429776.0         0.0  2269184.0    454.0         0         0   \n",
      "2157736  238429776.0         0.0  2269184.0    454.0         0         0   \n",
      "2157737  238429776.0         0.0  2269184.0    454.0         0         0   \n",
      "2157738  238429776.0         0.0  2269184.0    454.0         0         0   \n",
      "2157739  238429776.0         0.0  2269184.0    454.0         0         0   \n",
      "2157740  238429776.0         0.0  2269184.0    454.0         0         0   \n",
      "2157741  238429776.0         0.0  2269184.0    454.0         0         0   \n",
      "2157742  238429776.0         0.0  2269184.0    454.0         0         0   \n",
      "2157743  238429776.0         0.0  2269184.0    454.0         0         0   \n",
      "2157744  238429776.0         0.0  2269184.0    454.0         0         0   \n",
      "2157745  238429776.0         0.0  2269184.0    454.0         0         0   \n",
      "2157746  238429776.0         0.0  2269184.0    454.0         0         0   \n",
      "2157747  238429776.0         0.0  2269184.0    454.0         0         0   \n",
      "2157748  238429776.0         0.0  2269184.0    454.0         0         0   \n",
      "2157749  238429776.0         0.0  2269184.0    454.0         0         0   \n",
      "2157750  238429776.0         0.0  2269184.0    454.0         0         0   \n",
      "2157751  238429776.0         0.0  2269184.0    454.0         0         0   \n",
      "2157752  238429776.0         0.0  2269184.0    454.0         0         0   \n",
      "2157753  238429776.0         0.0  2269184.0    454.0         0         0   \n",
      "2157754  238429776.0         0.0  2269184.0    454.0         0         0   \n",
      "2157755  238429776.0         0.0  2269184.0    454.0         0         0   \n",
      "2157756  238429776.0         0.0  2269184.0    454.0         0         0   \n",
      "2157757  238429776.0         0.0  2269184.0    454.0         0         0   \n",
      "2157758  238429776.0         0.0  2269184.0    454.0         0         0   \n",
      "2157759  238429776.0         0.0  2269184.0    454.0         0         0   \n",
      "2157760  238429776.0         0.0  2269184.0    454.0         0         0   \n",
      "2157761  238429776.0         0.0  2269184.0    454.0         0         0   \n",
      "2157762  238429776.0         0.0  2269184.0    454.0         0         0   \n",
      "2157763  238429776.0         0.0  2269184.0    454.0         0         0   \n",
      "\n",
      "         POOLID_2  POOLID_3  POOLID_4  POOLID_5  POOLID_6  POOLID_7  \n",
      "0               0         0         0         0         0         0  \n",
      "1               0         0         0         0         0         0  \n",
      "2               0         0         0         0         0         0  \n",
      "3               0         0         0         0         0         0  \n",
      "4               0         0         0         0         0         0  \n",
      "5               0         0         0         0         0         0  \n",
      "6               0         0         0         0         0         0  \n",
      "7               0         0         0         0         0         0  \n",
      "8               0         0         0         0         0         0  \n",
      "9               0         0         0         0         0         0  \n",
      "10              0         0         0         0         0         0  \n",
      "11              0         0         0         0         0         0  \n",
      "12              0         0         0         0         0         0  \n",
      "13              0         0         0         0         0         0  \n",
      "14              0         0         0         0         0         0  \n",
      "15              0         0         0         0         0         0  \n",
      "16              0         0         0         0         0         0  \n",
      "17              0         0         0         0         0         0  \n",
      "18              0         0         0         0         0         0  \n",
      "19              0         0         0         0         0         0  \n",
      "20              0         0         0         0         0         0  \n",
      "21              0         0         0         0         0         0  \n",
      "22              0         0         0         0         0         0  \n",
      "23              0         0         0         0         0         0  \n",
      "24              0         0         0         0         0         0  \n",
      "25              0         0         0         0         0         0  \n",
      "26              0         0         0         0         0         0  \n",
      "27              0         0         0         0         0         0  \n",
      "28              0         0         0         0         0         0  \n",
      "29              0         0         0         0         0         0  \n",
      "...           ...       ...       ...       ...       ...       ...  \n",
      "2157734         1         0         0         0         0         0  \n",
      "2157735         1         0         0         0         0         0  \n",
      "2157736         1         0         0         0         0         0  \n",
      "2157737         1         0         0         0         0         0  \n",
      "2157738         1         0         0         0         0         0  \n",
      "2157739         1         0         0         0         0         0  \n",
      "2157740         1         0         0         0         0         0  \n",
      "2157741         1         0         0         0         0         0  \n",
      "2157742         1         0         0         0         0         0  \n",
      "2157743         1         0         0         0         0         0  \n",
      "2157744         1         0         0         0         0         0  \n",
      "2157745         1         0         0         0         0         0  \n",
      "2157746         1         0         0         0         0         0  \n",
      "2157747         1         0         0         0         0         0  \n",
      "2157748         1         0         0         0         0         0  \n",
      "2157749         1         0         0         0         0         0  \n",
      "2157750         1         0         0         0         0         0  \n",
      "2157751         1         0         0         0         0         0  \n",
      "2157752         1         0         0         0         0         0  \n",
      "2157753         1         0         0         0         0         0  \n",
      "2157754         1         0         0         0         0         0  \n",
      "2157755         1         0         0         0         0         0  \n",
      "2157756         1         0         0         0         0         0  \n",
      "2157757         1         0         0         0         0         0  \n",
      "2157758         1         0         0         0         0         0  \n",
      "2157759         1         0         0         0         0         0  \n",
      "2157760         1         0         0         0         0         0  \n",
      "2157761         1         0         0         0         0         0  \n",
      "2157762         1         0         0         0         0         0  \n",
      "2157763         1         0         0         0         0         0  \n",
      "\n",
      "[2157764 rows x 12 columns]\n"
     ]
    }
   ],
   "source": [
    "# One-hot encoding for the categories\n",
    "concatenated_dataset = pd.get_dummies(concatenated_dataset, columns=['POOLID']) \n",
    "values = list(concatenated_dataset.columns.values)\n",
    "print(concatenated_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            OBJID  ATTRLENGTH    BFSIZE   HDRSIZE\n",
      "0        0.094244         0.0  0.000047  0.110035\n",
      "1        0.094244         0.0  0.000047  0.110035\n",
      "2        0.094244         0.0  0.000047  0.110035\n",
      "3        0.094244         0.0  0.000047  0.110035\n",
      "4        0.094244         0.0  0.000047  0.110035\n",
      "5        0.094244         0.0  0.000047  0.110035\n",
      "6        0.094244         0.0  0.000047  0.110035\n",
      "7        0.094244         0.0  0.000047  0.110035\n",
      "8        0.094244         0.0  0.000047  0.110035\n",
      "9        0.094244         0.0  0.000047  0.110035\n",
      "10       0.094244         0.0  0.000047  0.110035\n",
      "11       0.094244         0.0  0.000047  0.110035\n",
      "12       0.094244         0.0  0.000047  0.110035\n",
      "13       0.094244         0.0  0.000047  0.110035\n",
      "14       0.094244         0.0  0.000047  0.110035\n",
      "15       0.094244         0.0  0.000047  0.110035\n",
      "16       0.094244         0.0  0.000047  0.110035\n",
      "17       0.094244         0.0  0.000047  0.110035\n",
      "18       0.094244         0.0  0.000047  0.110035\n",
      "19       0.094244         0.0  0.000047  0.110035\n",
      "20       0.094244         0.0  0.000047  0.110035\n",
      "21       0.094244         0.0  0.000047  0.110035\n",
      "22       0.094244         0.0  0.000047  0.110035\n",
      "23       0.094244         0.0  0.000047  0.110035\n",
      "24       0.094244         0.0  0.000047  0.110035\n",
      "25       0.094244         0.0  0.000047  0.110035\n",
      "26       0.094244         0.0  0.000047  0.110035\n",
      "27       0.094244         0.0  0.000047  0.110035\n",
      "28       0.094244         0.0  0.000047  0.110035\n",
      "29       0.094244         0.0  0.000047  0.110035\n",
      "...           ...         ...       ...       ...\n",
      "2157734  0.999562         0.0  0.000042  0.101730\n",
      "2157735  0.999562         0.0  0.000042  0.101730\n",
      "2157736  0.999562         0.0  0.000042  0.101730\n",
      "2157737  0.999562         0.0  0.000042  0.101730\n",
      "2157738  0.999562         0.0  0.000042  0.101730\n",
      "2157739  0.999562         0.0  0.000042  0.101730\n",
      "2157740  0.999562         0.0  0.000042  0.101730\n",
      "2157741  0.999562         0.0  0.000042  0.101730\n",
      "2157742  0.999562         0.0  0.000042  0.101730\n",
      "2157743  0.999562         0.0  0.000042  0.101730\n",
      "2157744  0.999562         0.0  0.000042  0.101730\n",
      "2157745  0.999562         0.0  0.000042  0.101730\n",
      "2157746  0.999562         0.0  0.000042  0.101730\n",
      "2157747  0.999562         0.0  0.000042  0.101730\n",
      "2157748  0.999562         0.0  0.000042  0.101730\n",
      "2157749  0.999562         0.0  0.000042  0.101730\n",
      "2157750  0.999562         0.0  0.000042  0.101730\n",
      "2157751  0.999562         0.0  0.000042  0.101730\n",
      "2157752  0.999562         0.0  0.000042  0.101730\n",
      "2157753  0.999562         0.0  0.000042  0.101730\n",
      "2157754  0.999562         0.0  0.000042  0.101730\n",
      "2157755  0.999562         0.0  0.000042  0.101730\n",
      "2157756  0.999562         0.0  0.000042  0.101730\n",
      "2157757  0.999562         0.0  0.000042  0.101730\n",
      "2157758  0.999562         0.0  0.000042  0.101730\n",
      "2157759  0.999562         0.0  0.000042  0.101730\n",
      "2157760  0.999562         0.0  0.000042  0.101730\n",
      "2157761  0.999562         0.0  0.000042  0.101730\n",
      "2157762  0.999562         0.0  0.000042  0.101730\n",
      "2157763  0.999562         0.0  0.000042  0.101730\n",
      "\n",
      "[2157764 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "#Normalization\n",
    "X_train = concatenated_dataset[values[:4]]\n",
    "X_train = ((X_train - X_train.min()) / (X_train.max() - X_train.min())).fillna(0)\n",
    "print(X_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[6.7989922e-01 0.0000000e+00 6.7710880e-06 1.7439446e-01]\n",
      " [7.0257801e-01 0.0000000e+00 8.3332066e-05 8.1660897e-02]\n",
      " [6.8140262e-01 0.0000000e+00 2.1553039e-06 1.8546712e-01]\n",
      " ...\n",
      " [7.0267153e-01 0.0000000e+00 9.1533664e-05 7.3356405e-02]\n",
      " [7.0264709e-01 0.0000000e+00 5.0144197e-05 6.5051906e-02]\n",
      " [6.3686877e-01 0.0000000e+00 1.0816575e-04 4.2214531e-02]]\n"
     ]
    }
   ],
   "source": [
    "#preprocess the data \n",
    "features = np.array(X_train, dtype='float32')\n",
    "target = np.array(concatenated_dataset[values[4:]], dtype='float32')\n",
    "\n",
    "# Shuffle Data\n",
    "indices = np.random.choice(len(features), len(features), replace=False)\n",
    "X_values = features[indices]\n",
    "y_values = target[indices]\n",
    "\n",
    "# Creating a Train and a Test Dataset\n",
    "test_size = 1000\n",
    "X_test = X_values[-test_size:]\n",
    "X_train = X_values[:-test_size]\n",
    "y_test = y_values[-test_size:]\n",
    "y_train = y_values[:-test_size]\n",
    "\n",
    "# print(X_train)\n",
    "print(X_test)\n",
    "# print(y_test)\n",
    "# print(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A neural Network which contains 3 layers with 4, 10, 8 nodes repectively was created!\n"
     ]
    }
   ],
   "source": [
    "# define a neural network\n",
    "\n",
    "# Initialize placeholders\n",
    "X_data = tf.placeholder(shape=[None, 4], dtype=tf.float32)\n",
    "y_target = tf.placeholder(shape=[None, 8], dtype=tf.float32)\n",
    "\n",
    "#create seed for random_normal()\n",
    "seed = 1234\n",
    "np.random.seed(seed)\n",
    "tf.set_random_seed(seed)\n",
    "\n",
    "hidden_layer_nodes = 10\n",
    "# We create a neural Network which contains 3 layers with 4, 8, 3 nodes repectively\n",
    "w1 = tf.Variable(tf.random_normal(shape=[4,hidden_layer_nodes])) # Weight of the input layer\n",
    "b1 = tf.Variable(tf.random_normal(shape=[hidden_layer_nodes]))   # Bias of the input layer\n",
    "w2 = tf.Variable(tf.random_normal(shape=[hidden_layer_nodes,8])) # Weight of the hidden layer\n",
    "b2 = tf.Variable(tf.random_normal(shape=[8]))                    # Bias of the hidden layer\n",
    "hidden_output = tf.nn.relu(tf.add(tf.matmul(X_data, w1), b1))\n",
    "final_output = tf.nn.softmax(tf.add(tf.matmul(hidden_output, w2), b2))\n",
    "\n",
    "# Loss Function\n",
    "loss = tf.reduce_mean(-tf.reduce_sum(y_target * tf.log(final_output), axis=0))\n",
    "\n",
    "# Optimizer\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.001).minimize(loss)\n",
    "\n",
    "print(\"A neural Network which contains 3 layers with 4, 10, 8 nodes repectively was created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the model...\n",
      "Epoch 100 | Loss: nan\n",
      "Epoch 200 | Loss: nan\n",
      "Epoch 300 | Loss: nan\n",
      "Epoch 400 | Loss: nan\n",
      "Epoch 500 | Loss: nan\n",
      "Epoch 600 | Loss: nan\n",
      "Epoch 700 | Loss: nan\n",
      "Epoch 800 | Loss: nan\n",
      "Epoch 900 | Loss: nan\n",
      "Epoch 1000 | Loss: nan\n",
      "Training finished\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Training the model...')\n",
    "\n",
    "# Interval / Epochs\n",
    "interval = 100\n",
    "epoch = 1000\n",
    "\n",
    "# Initialize variables\n",
    "sess = tf.Session()\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "\n",
    "# Training the model...\n",
    "for i in range(1, (epoch + 1)):\n",
    "    sess.run(optimizer, feed_dict={X_data: X_train, y_target: y_train})\n",
    "    if i % interval == 0:\n",
    "        print('Epoch', i, '|', 'Loss:', sess.run(loss, feed_dict={X_data: X_train, y_target: y_train}))\n",
    "\n",
    "print(\"Training finished\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of the model is: 0.215\n"
     ]
    }
   ],
   "source": [
    "# get the accuracy of the model\n",
    "correct_prediction = tf.equal(tf.argmax(final_output, 1), tf.argmax(y_target,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "print(\"The accuracy of the model is:\", sess.run(accuracy, feed_dict={X_data: X_test, y_target: y_test}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using model to predict pool id for features:  [[0.6934 0.     0.0029 0.1481]]\n",
      "\n",
      "Predicted softmax vector is:  [[nan nan nan nan nan nan nan nan]]\n",
      "\n",
      "Predicted pool id is:  POOLID_-1000000\n"
     ]
    }
   ],
   "source": [
    "# Prediction\n",
    "np.set_printoptions(precision=4)\n",
    "unknown = np.array([[0.693363, 0.0, 0.002894, 0.148097]], dtype=np.float32)\n",
    "predicted = sess.run(final_output, feed_dict={X_data: unknown})\n",
    "# model.predict(unknown)\n",
    "print(\"Using model to predict pool id for features: \", unknown)\n",
    "print(\"\\nPredicted softmax vector is: \",predicted)\n",
    "Class_dict={'POOLID_-1000000': 0, 'POOLID_-9': 1, 'POOLID_-1': 2, 'POOLID_4': 3, 'POOLID_-1': 4, 'POOLID_6': 5, 'POOLID_42': 6, 'POOLID_72': 7, 'POOLID_82': 8 }\n",
    "pool_dict = {v:k for k,v in Class_dict.items()}\n",
    "print(\"\\nPredicted pool id is: \", pool_dict[np.argmax(predicted)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
