{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PreProcessing CSV Files for use with Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.sql.functions import when   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initiates a SparkContext which is necessary for accessing data in Spark\n",
    "sc = SparkContext()\n",
    "sqlContext = SQLContext(sc)\n",
    "# change to match your environment\n",
    "# data_dir = \"Data/merge_data\"\n",
    "# just gonna keep it commented out instead of remove\n",
    "data_dir = '/home/cole/Workspace/School/Capstone/data/first_data_set/TestData/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All loaded at once: 10 Merge: 49\n"
     ]
    }
   ],
   "source": [
    "# this cell is not your part so don't worry about the issue\n",
    "# loading all of them does not do a proper merge, compare the columns\n",
    "df = sqlContext.read.format('com.databricks.spark.csv').option(\"header\", \"true\").load([data_dir +\"/SS_POOLS.csv\", data_dir +\"/SD_CHUNK_LOCATIONS.csv\", data_dir +\"/ARCHIVE_OBJECTS.csv\"])\n",
    "\n",
    "# need to load each individually then merge\n",
    "SDRO = sqlContext.read.format('com.databricks.spark.csv').option(\"header\", \"true\").load([data_dir + '/SD_RECON_ORDER.csv'])\n",
    "SS_POOLS = sqlContext.read.format('com.databricks.spark.csv').option(\"header\", \"true\").load([data_dir + '/SS_POOLS.csv'])\n",
    "AFBF = sqlContext.read.format('com.databricks.spark.csv').option(\"header\", \"true\").load([data_dir+\"/AF_BITFILES.csv\"])\n",
    "BACKUP_OBJECTS = sqlContext.read.format('com.databricks.spark.csv').option(\"header\", \"true\").load([data_dir+\"/BACKUP_OBJECTS.csv\"])\n",
    "\n",
    "# could probably be a one liner, one of the above not used for this example\n",
    "full_outer_join = BACKUP_OBJECTS.join(AFBF, BACKUP_OBJECTS.OBJID == AFBF.BFID,how='left')\n",
    "full_outer_join = full_outer_join.join(SDRO, ['OBJID'],how='left') \n",
    "\n",
    "print(\"All loaded at once: {} Merge: {}\".format(len(df.columns), len(full_outer_join.columns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset for memory\n",
    "SDRO = None\n",
    "SS_POOLS = None\n",
    "AFBF = None\n",
    "BACKUP_OBJECTS = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below takes the four columns that we want and writes casts them as integers. We then select each of the four features and write the new data frame to the desired folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# almost all operations create a copy so you need to assign\n",
    "# df.withColumn(\"POOLID\", df[\"POOLID\"].cast(\"int\"))\n",
    "# df.withColumn(\"SIZE\", df[\"SIZE\"].cast(\"int\"))\n",
    "# df.withColumn(\"OFFSET\", df[\"OFFSET\"].cast(\"int\"))\n",
    "# df.withColumn(\"LENGTH\", df[\"LENGTH\"].cast(\"int\"))\n",
    "\n",
    "# df.select(\"POOLID\", \"SIZE\", \"OFFSET\", \"LENGTH\").write.options(header='true').format('com.databricks.spark.csv').save(\"Data/merge_data/4_features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign\n",
    "df = full_outer_join # this line is pure laziness\n",
    "df = df.withColumn(\"POOLID\", df[\"POOLID\"].cast(\"int\"))\n",
    "df = df.withColumn(\"ATTRLENGTH\", df[\"ATTRLENGTH\"].cast(\"int\"))\n",
    "df = df.withColumn(\"BFSIZE\", df[\"BFSIZE\"].cast(\"int\"))\n",
    "df = df.withColumn(\"HDRSIZE\", df[\"HDRSIZE\"].cast(\"int\"))\n",
    "df = df.withColumn(\"OBJID\", df[\"OBJID\"].cast(\"int\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# probably the more important piece missed, need to remove gaps in data\n",
    "df = df.filter(df.POOLID. isNotNull())\n",
    "df = df.filter(df.ATTRLENGTH. isNotNull())\n",
    "df = df.filter(df.BFSIZE. isNotNull())\n",
    "df = df.filter(df.HDRSIZE. isNotNull())\n",
    "df = df.filter(df.OBJID. isNotNull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0, 1, 2, 3, 4, 5, 6, 7}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# another semi important part is to categorize every entry, it makes it easier as labels\n",
    "df = df.withColumn('POOLID', when(df['POOLID'] == -1000000, 0).otherwise(df['POOLID']))\n",
    "df = df.withColumn('POOLID', when(df['POOLID'] == -9, 1).otherwise(df['POOLID']))\n",
    "df = df.withColumn('POOLID', when(df['POOLID'] == 4, 2).otherwise(df['POOLID']))\n",
    "df = df.withColumn('POOLID', when(df['POOLID'] == 6, 3).otherwise(df['POOLID']))\n",
    "df = df.withColumn('POOLID', when(df['POOLID'] == 42, 4).otherwise(df['POOLID']))\n",
    "df = df.withColumn('POOLID', when(df['POOLID'] == 72, 5).otherwise(df['POOLID']))\n",
    "df = df.withColumn('POOLID', when(df['POOLID'] == 82, 6).otherwise(df['POOLID']))\n",
    "df = df.withColumn('POOLID', when(df['POOLID'] == -1, 7).otherwise(df['POOLID']))\n",
    "\n",
    "#after\n",
    "li = df.select(\"POOLID\").rdd.flatMap(lambda x: x).collect()\n",
    "set(li) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select(\"OBJID\", \"ATTRLENGTH\", \"BFSIZE\", \"HDRSIZE\", \"POOLID\").write.options(header='true').format('com.databricks.spark.csv').save(data_dir + \"/merge_data/4_features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below then goes and reads the CSV files generated by the cell above and prints out all the new column names and types to make sure we outputted what we wanted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['OBJID', 'ATTRLENGTH', 'BFSIZE', 'HDRSIZE', 'POOLID']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df = sqlContext.read.format('com.databricks.spark.csv').option(\"header\", \"true\").load(data_dir + \"/merge_data/4_features/*.csv\")\n",
    "test_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
