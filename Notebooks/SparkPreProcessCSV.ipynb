{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PreProcessing CSV Files for use with Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.types import IntegerType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initiates a SparkContext which is necessary for accessing data in Spark\n",
    "sc = SparkContext()\n",
    "sqlContext = SQLContext(sc)\n",
    "# change to match your environment\n",
    "output_dir = \"Data/merge_data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = sqlContext.read.format('com.databricks.spark.csv').option(\"header\", \"true\").load([\"Data/SS_POOLS.csv\", \"Data/SD_CHUNK_LOCATIONS.csv\", \"Data/ARCHIVE_OBJECTS.csv\"])\n",
    "\n",
    "# df.printSchema()\n",
    "# df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below takes the four columns that we want and writes casts them as integers. We then select each of the four features and write the new data frame to the desired folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.withColumn(\"POOLID\", df[\"POOLID\"].cast(\"int\"))\n",
    "df.withColumn(\"SIZE\", df[\"SIZE\"].cast(\"int\"))\n",
    "df.withColumn(\"OFFSET\", df[\"OFFSET\"].cast(\"int\"))\n",
    "df.withColumn(\"LENGTH\", df[\"LENGTH\"].cast(\"int\"))\n",
    "\n",
    "df.select(\"POOLID\", \"SIZE\", \"OFFSET\", \"LENGTH\").write.options(header='true').format('com.databricks.spark.csv').save(\"Data/merge_data/4_features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below then goes and reads the CSV files generated by the cell above and prints out all the new column names and types to make sure we outputted what we wanted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['POOLID', 'SIZE', 'OFFSET', 'LENGTH']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df = sqlContext.read.format('com.databricks.spark.csv').option(\"header\", \"true\").load(\"Data/merge_data/4_features/*.csv\")\n",
    "test_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
