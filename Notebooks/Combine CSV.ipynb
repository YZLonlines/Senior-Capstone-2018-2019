{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combine CSVs For Processing\n",
    "Combines the CSVs so all relevant data is on one line and the data is placed based on linking keys through the different CSVs\n",
    "\n",
    "Everything from this markdown cell to the next one is just a sanity check to make sure the chunking for the CSVs is not giving an odd output. Don't run unless you have some reason to check."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import dask.dataframe as dd\n",
    "data_dir = '/home/cole/Workspace/School/Capstone/data/first_data_set/TestData/'"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "backup_objects = dd.read_csv(data_dir + 'BACKUP_OBJECTS.csv', dtype={'DEACDATE': 'str','META_UPDATE': 'str','ATTRLENGTH': 'float64','FLAGS': 'float64','STG_HINT': 'float64'})"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "AFBF = dd.read_csv(data_dir + 'AF_BITFILES.csv')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df = backup_objects.merge(AFBF, how='left', left_on ='OBJID', right_on= 'BFID')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "backup_objects_panda = pd.read_csv(data_dir + 'BACKUP_OBJECTS.csv')\n",
    "AFBF_panda = pd.read_csv(data_dir + 'AF_BITFILES.csv')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df2 = backup_objects_panda.merge(AFBF_panda, how='left', left_on ='OBJID', right_on= 'BFID')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "index_pairs = []\n",
    "for i in AFBF_panda['BFID']:\n",
    "    ret = backup_objects_panda['OBJID'][backup_objects_panda['OBJID']==i]\n",
    "    if len(ret):\n",
    "        index_pairs.append(ret)\n",
    "    if len(index_pairs) >= 10:\n",
    "        break\n",
    "index_pairs"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "backup_objects_panda['OBJID'][backup_objects_panda['OBJID']==218050714]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "AFBF_panda['BFID'][AFBF_panda['BFID']==218050714]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df2.shape"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "AFBF_panda.shape"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "backup_objects_panda.shape"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "backup_objects_panda.iloc[2085893]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "AFBF_panda.iloc[14]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "index_pairs = []\n",
    "for i in AFBF_panda['BFID']:\n",
    "    ret = df2['OBJID'][df2['OBJID']==i]\n",
    "    if len(ret):\n",
    "        index_pairs.append(ret)\n",
    "    if len(index_pairs) >= 10:\n",
    "        break\n",
    "index_pairs"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df2.iloc[2085939]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "backup_objects_panda = []\n",
    "AFBF_panda = []"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df2.head()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df2.columns"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "row = []\n",
    "for label in df.columns:\n",
    "    row.append((label, df[label].compute().iloc[2085939]))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "for i in range(len(df.columns)):\n",
    "    try:\n",
    "        assert df2.iloc[2085939][df.columns[i]] == row[i][1] or (np.isnan(df2.iloc[2085939][df.columns[i]]) and np.isnan(row[i][1]))\n",
    "    except:\n",
    "        print('{} != {} at index : {}'.format(df2.iloc[2085939][df.columns[i]], row[i][1], i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combine with Dask\n",
    "\n",
    "We use dask because it automatically chunks data\n",
    "\n",
    "We need to determine what features we want to be able to determine which CSVs we need to merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cole/miniconda3/envs/capstone/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/home/cole/miniconda3/envs/capstone/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import dask.dataframe as dd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change to be reflective of your environment\n",
    "data_dir = '/home/cole/Workspace/School/TestData/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "BACKUP_OBJECTS = dd.read_csv(data_dir + 'BACKUP_OBJECTS.csv', dtype = {'META_UPDATE': 'str', 'DEACDATE': 'str','ATTRLENGTH': 'float64','FLAGS': 'float64','STG_HINT': 'float64', 'OWNER' : 'str'})\n",
    "AFBF = dd.read_csv(data_dir + 'AF_BITFILES.csv')\n",
    "SDRO = dd.read_csv(data_dir + 'SD_RECON_ORDER.csv')\n",
    "SS_POOLS = dd.read_csv(data_dir + 'SS_POOLS.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = BACKUP_OBJECTS.merge(AFBF, how='left', left_on ='OBJID', right_on= 'BFID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.merge(SDRO, how='left', left_on ='OBJID', right_on= 'OBJID' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_parquet(data_dir + 'merged_data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
