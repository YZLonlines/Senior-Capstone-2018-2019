{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change to be reflective of your environment\n",
    "data_dir = '/Users/zhaoluyang/Downloads/Senior-Capstone-2018-2019-master/Notebooks/TestData'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "size = os.path.getsize(data_dir+\"/BACKUP_OBJECTS.csv\")\n",
    "end = 50 # how many gigs to scale backup objects\n",
    "upscale = int((end * 1073741824) / size)\n",
    "upscale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot run multiple SparkContexts at once; existing SparkContext(app=pyspark-shell, master=local[*]) created by getOrCreate at <ipython-input-1-a5c6d49f12a3>:138 ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-2bfa00756323>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# initiates a SparkContext which is necessary for accessing data in Spark\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0msc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0msqlContext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSQLContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# change to match your environment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0moutput_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_dir\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"/merge_data\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.6/site-packages/pyspark/context.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls)\u001b[0m\n\u001b[1;32m    113\u001b[0m         \"\"\"\n\u001b[1;32m    114\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callsite\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfirst_spark_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mCallSite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m         \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgateway\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m             self._do_init(master, appName, sparkHome, pyFiles, environment, batchSize, serializer,\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.6/site-packages/pyspark/context.py\u001b[0m in \u001b[0;36m_ensure_initialized\u001b[0;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[1;32m    312\u001b[0m                         \u001b[0;34m\" created by %s at %s:%s \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m                         % (currentAppName, currentMaster,\n\u001b[0;32m--> 314\u001b[0;31m                             callsite.function, callsite.file, callsite.linenum))\n\u001b[0m\u001b[1;32m    315\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m                     \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minstance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot run multiple SparkContexts at once; existing SparkContext(app=pyspark-shell, master=local[*]) created by getOrCreate at <ipython-input-1-a5c6d49f12a3>:138 "
     ]
    }
   ],
   "source": [
    "# initiates a SparkContext which is necessary for accessing data in Spark\n",
    "sc = SparkContext()\n",
    "sqlContext = SQLContext(sc)\n",
    "# change to match your environment\n",
    "output_dir = data_dir + \"/merge_data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SDRO = sqlContext.read.format('com.databricks.spark.csv').option(\"header\", \"true\").load([data_dir + '/SD_RECON_ORDER.csv'])\n",
    "SS_POOLS = sqlContext.read.format('com.databricks.spark.csv').option(\"header\", \"true\").load([data_dir + '/SS_POOLS.csv'])\n",
    "AFBF = sqlContext.read.format('com.databricks.spark.csv').option(\"header\", \"true\").load([data_dir+\"/AF_BITFILES.csv\"])\n",
    "BACKUP_OBJECTS = sqlContext.read.format('com.databricks.spark.csv').option(\"header\", \"true\").load([data_dir+\"/BACKUP_OBJECTS.csv\"])\n",
    "ls = [SDRO, SS_POOLS, AFBF, BACKUP_OBJECTS]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# upscale step\n",
    "for i in range(len(ls)):\n",
    "    csv = ls[i]\n",
    "    concat = csv\n",
    "    print(concat.count())\n",
    "    for j in range(upscale):\n",
    "        concat = concat.union(csv)\n",
    "    ls[i] = concat\n",
    "    print(concat.count())\n",
    "    \n",
    "    \n",
    "SDRO = ls[0]\n",
    "SS_POOLS = ls[1]\n",
    "AFBF = ls[2]\n",
    "BACKUP_OBJECTS = ls[3]\n",
    "\n",
    "# # save new ones before join then reload\n",
    "# SDRO.write.options(header='true').format('com.databricks.spark.csv').save(data_dir + \"/upscale_\" + str(end) + \"_SDRO\")\n",
    "# SS_POOLS.write.options(header='true').format('com.databricks.spark.csv').save(data_dir + \"/upscale_\" + str(end) + \"_SS_POOLS\")\n",
    "# AFBF.write.options(header='true').format('com.databricks.spark.csv').save(data_dir + \"/upscale_\" + str(end) + \"_AFBF\")\n",
    "# BACKUP_OBJECTS.write.options(header='true').format('com.databricks.spark.csv').save(data_dir + \"/upscale_\" + str(end) + \"_BACKUP_OBJECTS\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SDRO = None\n",
    "# SS_POOLS = None\n",
    "# AFBF = None\n",
    "# BACKUP_OBJECTS = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SDRO = sqlContext.read.format('com.databricks.spark.csv').option(\"header\", \"true\").load(data_dir + \"/upscale_\" + str(end) + \"_SDRO/*.csv\")\n",
    "# SS_POOLS = sqlContext.read.format('com.databricks.spark.csv').option(\"header\", \"true\").load(data_dir + \"/upscale_\" + str(end) + \"_SS_POOLS/*.csv\")\n",
    "# AFBF = sqlContext.read.format('com.databricks.spark.csv').option(\"header\", \"true\").load(data_dir + \"/upscale_\" + str(end) + \"_AFBF/*.csv\")\n",
    "# BACKUP_OBJECTS = sqlContext.read.format('com.databricks.spark.csv').option(\"header\", \"true\").load(data_dir + \"/upscale_\" + str(end) + \"_BACKUP_OBJECTS/*.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_outer_join = BACKUP_OBJECTS.join(AFBF, BACKUP_OBJECTS.OBJID == AFBF.BFID,how='left') # Could also use 'full_outer'\n",
    "full_outer_join = full_outer_join.join(SDRO, ['OBJID'],how='left') # Could also use 'full_outer'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SDRO = None\n",
    "SS_POOLS = None\n",
    "AFBF = None\n",
    "BACKUP_OBJECTS = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = full_outer_join\n",
    "df = df.filter(df.POOLID. isNotNull())\n",
    "df = df.withColumn(\"POOLID\", df[\"POOLID\"].cast(\"int\"))\n",
    "df = df.filter(df.ATTRLENGTH. isNotNull())\n",
    "df = df.withColumn(\"ATTRLENGTH\", df[\"ATTRLENGTH\"].cast(\"float\"))\n",
    "df = df.filter(df.BFSIZE. isNotNull())\n",
    "df = df.withColumn(\"BFSIZE\", df[\"BFSIZE\"].cast(\"float\"))\n",
    "df = df.filter(df.HDRSIZE. isNotNull())\n",
    "df = df.withColumn(\"HDRSIZE\", df[\"HDRSIZE\"].cast(\"float\"))\n",
    "df = df.filter(df.OBJID. isNotNull())\n",
    "df = df.withColumn(\"OBJID\", df[\"OBJID\"].cast(\"float\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when   \n",
    "\n",
    "df = df.withColumn('POOLID', when(df['POOLID'] == -1000000, 0).otherwise(df['POOLID']))\n",
    "df = df.withColumn('POOLID', when(df['POOLID'] == -9, 1).otherwise(df['POOLID']))\n",
    "df = df.withColumn('POOLID', when(df['POOLID'] == 4, 2).otherwise(df['POOLID']))\n",
    "df = df.withColumn('POOLID', when(df['POOLID'] == 6, 3).otherwise(df['POOLID']))\n",
    "df = df.withColumn('POOLID', when(df['POOLID'] == 42, 4).otherwise(df['POOLID']))\n",
    "df = df.withColumn('POOLID', when(df['POOLID'] == 72, 5).otherwise(df['POOLID']))\n",
    "df = df.withColumn('POOLID', when(df['POOLID'] == 82, 6).otherwise(df['POOLID']))\n",
    "df = df.withColumn('POOLID', when(df['POOLID'] == -1, 7).otherwise(df['POOLID']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.filter(df.POOLID. isNotNull())\n",
    "df = df.withColumn(\"POOLID\", df[\"POOLID\"].cast(\"int\"))\n",
    "df = df.filter(df.ATTRLENGTH. isNotNull())\n",
    "df = df.withColumn(\"ATTRLENGTH\", df[\"ATTRLENGTH\"].cast(\"float\"))\n",
    "df = df.filter(df.BFSIZE. isNotNull())\n",
    "df = df.withColumn(\"BFSIZE\", df[\"BFSIZE\"].cast(\"float\"))\n",
    "df = df.filter(df.HDRSIZE. isNotNull())\n",
    "df = df.withColumn(\"HDRSIZE\", df[\"HDRSIZE\"].cast(\"float\"))\n",
    "df = df.filter(df.OBJID. isNotNull())\n",
    "df = df.withColumn(\"OBJID\", df[\"OBJID\"].cast(\"float\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select(\"ATTRLENGTH\", \"BFSIZE\", \"HDRSIZE\", \"POOLID\").write.options(header='true').format('com.databricks.spark.csv').save(data_dir + \"/merge_data/upscale\")\n",
    "                                                                                                    \n",
    "                                                                                                                                                   \n",
    "                                                                                                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependencies\n",
    "import glob, os\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# create seed for random_normal()\n",
    "seed = 1234\n",
    "np.random.seed(seed)\n",
    "tf.set_random_seed(seed)\n",
    "input_nodes = 5\n",
    "output_nodes = 3\n",
    "hidden_layer_nodes = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data preprocess\n",
    "def data_preprocess(next):\n",
    "    tf_features = [next['BFSIZE'], next['NODESTATE'], next['METADATASIZE'], next['STG_HINT'], next['BFSIZE']]\n",
    "    # tf_features = [next['BFSIZE'], next['HDRSIZE'], next['METADATASIZE']]\n",
    "    features_columns = np.array(sess.run(tf_features), dtype='float32')\n",
    "\n",
    "    num_features = len(tf_features)\n",
    "    # Normalization\n",
    "    for column in range(num_features):\n",
    "        column_range = features_columns[column].max() - features_columns[column].min()\n",
    "        # print(column_range)\n",
    "        if column_range != 0.0:\n",
    "            features_columns[column] = (features_columns[column] - features_columns[column].min()) / column_range\n",
    "\n",
    "    features = np.array([]).reshape(0, num_features)\n",
    "    for i in range(batch_len):\n",
    "        ele = np.zeros(num_features)\n",
    "        for j in range(num_features):\n",
    "            np.put(ele, j, features_columns[j][i])\n",
    "        features = np.r_[features, [ele]]\n",
    "    # print(len(features))\n",
    "\n",
    "    tf_labels = next['label']\n",
    "    labels = np.array(sess.run(tf_labels), dtype='float32')\n",
    "    # print(labels)\n",
    "\n",
    "    # One-hot encoding for the categories\n",
    "    num_classes = 3\n",
    "    targets = np.array([]).reshape(0, num_classes)\n",
    "\n",
    "    for i in range(0, batch_len):\n",
    "        ele = np.zeros(num_classes)\n",
    "        np.put(ele, labels[i], 1)\n",
    "        targets = np.r_[targets, [ele]]\n",
    "\n",
    "    #     return features, targets\n",
    "\n",
    "    #     Shuffle Data\n",
    "    indices = np.random.choice(len(features), len(features), replace=False)\n",
    "    X_data = features[indices]\n",
    "    y_data = targets[indices]\n",
    "\n",
    "    # X_values = features[indices]\n",
    "    # y_values = targets[indices]\n",
    "\n",
    "    return X_data, y_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(X_train, y_train, start_epoch):\n",
    "    # Interval / Epochs\n",
    "    interval = 500\n",
    "    epoch = 1500\n",
    "\n",
    "    # Training the model...\n",
    "    for i in range(1, (epoch + 1)):\n",
    "        sess.run(optimizer, feed_dict={X_data: X_train, y_target: y_train})\n",
    "        if i % interval == 0:\n",
    "            print('Epoch', i + start_epoch, '|', 'Loss:',\n",
    "                  sess.run(loss, feed_dict={X_data: X_train, y_target: y_train}))\n",
    "            # print(sess.run(b1))\n",
    "\n",
    "    return i + start_epoch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the accuracy of the model\n",
    "def predict(X_test, y_test):\n",
    "    correct_prediction = tf.equal(tf.argmax(final_output, 1), tf.argmax(y_target,1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    print(\"The accuracy of the model is: \", sess.run(accuracy, feed_dict={X_data: X_test, y_target: y_test}))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A neural Network which contains 3 layers with 5 ,  10 ,  3  nodes repectively was created!\n"
     ]
    }
   ],
   "source": [
    "# define a neural network\n",
    "\n",
    "# Initialize placeholders\n",
    "X_data = tf.placeholder(shape=[None, input_nodes], dtype=tf.float32)\n",
    "y_target = tf.placeholder(shape=[None, output_nodes], dtype=tf.float32)\n",
    "\n",
    "# We create a neural Network which contains 3 layers with 4, 10, 5 nodes repectively\n",
    "w1 = tf.Variable(tf.random_normal(shape=[input_nodes, hidden_layer_nodes]))  # Weight of the input layer\n",
    "b1 = tf.Variable(tf.random_normal(shape=[hidden_layer_nodes]))  # Bias of the input layer\n",
    "w2 = tf.Variable(tf.random_normal(shape=[hidden_layer_nodes, output_nodes]))  # Weight of the hidden layer\n",
    "b2 = tf.Variable(tf.random_normal(shape=[output_nodes]))  # Bias of the hidden layer\n",
    "hidden_output = tf.nn.relu(tf.add(tf.matmul(X_data, w1), b1))\n",
    "final_output = tf.nn.softmax(tf.add(tf.matmul(hidden_output, w2), b2))\n",
    "\n",
    "# Loss Function\n",
    "loss = tf.reduce_mean(-tf.reduce_sum(y_target * tf.log(final_output), axis=0))\n",
    "\n",
    "# Optimizer\n",
    "# optimizer = tf.train.RMSPropOptimizer(learning_rate=0.001).minimize(loss)\n",
    "# optimizer = tf.train.ProximalGradientDescentOptimizer(learning_rate=0.001).minimize(loss)\n",
    "# optimizer = tf.train.ProximalAdagradOptimizer(learning_rate=0.01).minimize(loss)\n",
    "# optimizer = tf.train.FtrlOptimizer(learning_rate=0.01).minimize(loss)\n",
    "optimizer = tf.train.AdagradOptimizer(learning_rate=0.1).minimize(loss)\n",
    "# optimizer = tf.train.AdamOptimizer(learning_rate=0.001).minimize(loss)\n",
    "# optimizer = tf.train.AdadeltaOptimizer(learning_rate=0.01).minimize(loss)\n",
    "# optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01).minimize(loss)\n",
    "\n",
    "print(\"A neural Network which contains 3 layers with\", input_nodes, \", \", hidden_layer_nodes, \", \", output_nodes,\n",
    "      \" nodes repectively was created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the model...\n",
      "Epoch 500 | Loss: 686.616\n",
      "Epoch 1000 | Loss: 686.0283\n",
      "Epoch 1500 | Loss: 685.7827\n",
      "Epoch 2000 | Loss: 692.50946\n",
      "Epoch 2500 | Loss: 692.18463\n",
      "Epoch 3000 | Loss: 692.01764\n",
      "Epoch 3500 | Loss: 685.9758\n",
      "Epoch 4000 | Loss: 685.7102\n",
      "Epoch 4500 | Loss: 685.53534\n",
      "Epoch 5000 | Loss: 630.41766\n",
      "Epoch 5500 | Loss: 630.2012\n",
      "Epoch 6000 | Loss: 630.06146\n",
      "Epoch 6500 | Loss: 687.795\n",
      "Epoch 7000 | Loss: 687.66797\n",
      "Epoch 7500 | Loss: 687.5865\n",
      "Epoch 8000 | Loss: 680.63336\n",
      "Epoch 8500 | Loss: 680.1218\n",
      "Epoch 9000 | Loss: 679.8581\n",
      "Epoch 9500 | Loss: 690.70074\n",
      "Epoch 10000 | Loss: 690.49896\n",
      "Epoch 10500 | Loss: 690.33716\n",
      "Epoch 11000 | Loss: 620.8725\n",
      "Epoch 11500 | Loss: 620.4439\n",
      "Epoch 12000 | Loss: 620.29083\n",
      "Epoch 12500 | Loss: 691.8057\n",
      "Epoch 13000 | Loss: 691.4412\n",
      "Epoch 13500 | Loss: 691.22845\n",
      "Epoch 14000 | Loss: 679.8381\n",
      "Epoch 14500 | Loss: 679.5461\n",
      "Epoch 15000 | Loss: 679.3776\n",
      "Epoch 15500 | Loss: 672.19635\n",
      "Epoch 16000 | Loss: 671.8801\n",
      "Epoch 16500 | Loss: 671.79877\n",
      "Epoch 17000 | Loss: 659.3924\n",
      "Epoch 17500 | Loss: 659.30975\n",
      "Epoch 18000 | Loss: 659.27057\n",
      "Epoch 18500 | Loss: 690.0801\n",
      "Epoch 19000 | Loss: 689.9434\n",
      "Epoch 19500 | Loss: 689.8896\n",
      "Epoch 20000 | Loss: 683.0047\n",
      "Epoch 20500 | Loss: 682.8785\n",
      "Epoch 21000 | Loss: 682.825\n",
      "Epoch 21500 | Loss: 648.7376\n",
      "Epoch 22000 | Loss: 648.5821\n",
      "Epoch 22500 | Loss: 648.5184\n",
      "Epoch 23000 | Loss: 684.9497\n",
      "Epoch 23500 | Loss: 684.5754\n",
      "Epoch 24000 | Loss: 684.2917\n",
      "Epoch 24500 | Loss: 691.1109\n",
      "Epoch 25000 | Loss: 690.55493\n",
      "Epoch 25500 | Loss: 690.446\n",
      "Epoch 26000 | Loss: 692.49854\n",
      "Epoch 26500 | Loss: 692.3108\n",
      "Epoch 27000 | Loss: 692.21185\n",
      "Epoch 27500 | Loss: 614.9528\n",
      "Epoch 28000 | Loss: 614.6798\n",
      "Epoch 28500 | Loss: 614.56256\n",
      "Epoch 29000 | Loss: 691.545\n",
      "Epoch 29500 | Loss: 691.35864\n",
      "Epoch 30000 | Loss: 691.2093\n",
      "The accuracy of the model is:  0.525\n",
      "Training finished\n",
      "\n"
     ]
    }
   ],
   "source": [
    "path = '/Users/zhaoluyang/Downloads/Senior-Capstone-2018-2019-master/Notebooks/firstTestData/merge_data/feature_extraction'\n",
    "# path = data_dir + \"/merge_data/upscale\"\n",
    "all_files = glob.glob(\n",
    "    os.path.join(path, \"*.csv\"))  # advisable to use os.path.join as this makes concatenation OS independent\n",
    "\n",
    "df_from_each_file = (pd.read_csv(f) for f in all_files)\n",
    "concatenated_dataset = pd.concat(df_from_each_file, ignore_index=True)\n",
    "# 44633877\n",
    "# print(len(concatenated_dataset))\n",
    "\n",
    "batch_len = 4000\n",
    "dataset = tf.data.experimental.make_csv_dataset(all_files, batch_size=batch_len)\n",
    "# print(dataset)\n",
    "\n",
    "start_epoch = 0\n",
    "print('Training the model...')\n",
    "iter = dataset.make_one_shot_iterator()\n",
    "\n",
    "# Initialize variables\n",
    "sess = tf.Session()\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "\n",
    "# There are total 2,157,764 rows in the combined csv files\n",
    "\n",
    "# choose first 4000 rows as test data\n",
    "next = iter.get_next()\n",
    "\n",
    "X_test, y_test = data_preprocess(next)\n",
    "\n",
    "# then 538*4000 = 2,152,000 rows will be used as training data\n",
    "# 538\n",
    "for i in range(20):\n",
    "    next = iter.get_next()\n",
    "    # next is a dict with key=columns names and value=column data\n",
    "    X_train, y_train = data_preprocess(next)\n",
    "    #     print(X_train)\n",
    "    start_epoch = training(X_train, y_train, start_epoch)\n",
    "\n",
    "predict(X_test, y_test)\n",
    "\n",
    "print(\"Training finished\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction\n",
    "np.set_printoptions(precision=4)\n",
    "unknown = np.array([[0.0, 0.002894, 0.148097]], dtype=np.float32)\n",
    "predicted = sess.run(final_output, feed_dict={X_data: unknown})\n",
    "# model.predict(unknown)\n",
    "print(\"Using model to predict pool id for features: \", unknown)\n",
    "print(\"\\nPredicted softmax vector is: \",predicted)\n",
    "Class_dict={'POOLID_0': 0, 'POOLID_1': 1, 'POOLID_2': 2}\n",
    "pool_dict = {v:k for k,v in Class_dict.items()}\n",
    "print(\"\\nPredicted pool id is: \", pool_dict[np.argmax(predicted)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"strided_slice_1:0\", shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
