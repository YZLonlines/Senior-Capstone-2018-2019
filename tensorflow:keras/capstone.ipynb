{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iris dataset using Keras/TensorFlow \n",
      "Loading Iris data into memory\n",
      "Starting training \n",
      "Epoch 1/100\n",
      "105/105 [==============================] - 2s 17ms/step - loss: 1.4178 - acc: 0.4667\n",
      "Epoch 2/100\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 1.0151 - acc: 0.4857\n",
      "Epoch 3/100\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 0.8457 - acc: 0.4857\n",
      "Epoch 4/100\n",
      "105/105 [==============================] - 0s 3ms/step - loss: 0.7486 - acc: 0.6857\n",
      "Epoch 5/100\n",
      "105/105 [==============================] - 0s 3ms/step - loss: 0.6871 - acc: 0.8667\n",
      "Epoch 6/100\n",
      "105/105 [==============================] - 0s 3ms/step - loss: 0.6413 - acc: 0.8762\n",
      "Epoch 7/100\n",
      "105/105 [==============================] - 0s 3ms/step - loss: 0.6015 - acc: 0.8952\n",
      "Epoch 8/100\n",
      "105/105 [==============================] - 0s 3ms/step - loss: 0.5699 - acc: 0.8857\n",
      "Epoch 9/100\n",
      "105/105 [==============================] - 0s 3ms/step - loss: 0.5341 - acc: 0.9238\n",
      "Epoch 10/100\n",
      "105/105 [==============================] - 0s 3ms/step - loss: 0.4929 - acc: 0.9238\n",
      "Epoch 11/100\n",
      "105/105 [==============================] - 0s 3ms/step - loss: 0.4575 - acc: 0.9429\n",
      "Epoch 12/100\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 0.4027 - acc: 0.9333\n",
      "Epoch 13/100\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 0.3749 - acc: 0.9333\n",
      "Epoch 14/100\n",
      "105/105 [==============================] - 0s 3ms/step - loss: 0.3247 - acc: 0.9238\n",
      "Epoch 15/100\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 0.2914 - acc: 0.9429\n",
      "Epoch 16/100\n",
      "105/105 [==============================] - 0s 3ms/step - loss: 0.2669 - acc: 0.9429\n",
      "Epoch 17/100\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 0.2530 - acc: 0.9714\n",
      "Epoch 18/100\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 0.2240 - acc: 0.9524\n",
      "Epoch 19/100\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 0.2144 - acc: 0.9524\n",
      "Epoch 20/100\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 0.1966 - acc: 0.9524\n",
      "Epoch 21/100\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 0.1938 - acc: 0.9524\n",
      "Epoch 22/100\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 0.1799 - acc: 0.9524\n",
      "Epoch 23/100\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 0.1776 - acc: 0.9429\n",
      "Epoch 24/100\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 0.1630 - acc: 0.9619\n",
      "Epoch 25/100\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 0.1601 - acc: 0.9524\n",
      "Epoch 26/100\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 0.1511 - acc: 0.9619\n",
      "Epoch 27/100\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 0.1494 - acc: 0.9429\n",
      "Epoch 28/100\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 0.1403 - acc: 0.9619\n",
      "Epoch 29/100\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 0.1489 - acc: 0.9524\n",
      "Epoch 30/100\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 0.1577 - acc: 0.9524\n",
      "Epoch 31/100\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 0.1379 - acc: 0.9429\n",
      "Epoch 32/100\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 0.1416 - acc: 0.9619\n",
      "Epoch 33/100\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 0.1413 - acc: 0.9619\n",
      "Epoch 34/100\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 0.1303 - acc: 0.9524\n",
      "Epoch 35/100\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 0.1315 - acc: 0.9714\n",
      "Epoch 36/100\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 0.1293 - acc: 0.9524\n",
      "Epoch 37/100\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 0.1197 - acc: 0.9429\n",
      "Epoch 38/100\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 0.1220 - acc: 0.9619\n",
      "Epoch 39/100\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 0.1287 - acc: 0.9524\n",
      "Epoch 40/100\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 0.1311 - acc: 0.9714\n",
      "Epoch 41/100\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 0.1169 - acc: 0.9619\n",
      "Epoch 42/100\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 0.1134 - acc: 0.9619\n",
      "Epoch 43/100\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 0.1116 - acc: 0.9714\n",
      "Epoch 44/100\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 0.1104 - acc: 0.9524\n",
      "Epoch 45/100\n",
      "105/105 [==============================] - 0s 3ms/step - loss: 0.1072 - acc: 0.9714\n",
      "Epoch 46/100\n",
      "105/105 [==============================] - 0s 3ms/step - loss: 0.1107 - acc: 0.9429\n",
      "Epoch 47/100\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 0.1086 - acc: 0.9714\n",
      "Epoch 48/100\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 0.1160 - acc: 0.9619\n",
      "Epoch 49/100\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 0.1049 - acc: 0.9714\n",
      "Epoch 50/100\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 0.1052 - acc: 0.9619\n",
      "Epoch 51/100\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 0.0994 - acc: 0.9619\n",
      "Epoch 52/100\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 0.1020 - acc: 0.9619\n",
      "Epoch 53/100\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 0.1042 - acc: 0.9619\n",
      "Epoch 54/100\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 0.1067 - acc: 0.9619\n",
      "Epoch 55/100\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 0.0929 - acc: 0.9714\n",
      "Epoch 56/100\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 0.0857 - acc: 0.9619\n",
      "Epoch 57/100\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 0.1014 - acc: 0.9619\n",
      "Epoch 58/100\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 0.0843 - acc: 0.9619\n",
      "Epoch 59/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.1153 - acc: 0.961 - 0s 5ms/step - loss: 0.1146 - acc: 0.9619\n",
      "Epoch 60/100\n",
      "105/105 [==============================] - 1s 5ms/step - loss: 0.0998 - acc: 0.9524\n",
      "Epoch 61/100\n",
      "105/105 [==============================] - 0s 3ms/step - loss: 0.0962 - acc: 0.9714\n",
      "Epoch 62/100\n",
      "105/105 [==============================] - 1s 8ms/step - loss: 0.0883 - acc: 0.9524\n",
      "Epoch 63/100\n",
      "105/105 [==============================] - 0s 3ms/step - loss: 0.0936 - acc: 0.9714\n",
      "Epoch 64/100\n",
      "105/105 [==============================] - 0s 3ms/step - loss: 0.0879 - acc: 0.9714\n",
      "Epoch 65/100\n",
      "105/105 [==============================] - 0s 3ms/step - loss: 0.0938 - acc: 0.9619\n",
      "Epoch 66/100\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 0.0882 - acc: 0.9714\n",
      "Epoch 67/100\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 0.0959 - acc: 0.9619\n",
      "Epoch 68/100\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 0.0831 - acc: 0.9619\n",
      "Epoch 69/100\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 0.0808 - acc: 0.9714\n",
      "Epoch 70/100\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 0.0837 - acc: 0.9524\n",
      "Epoch 71/100\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 0.0960 - acc: 0.9619\n",
      "Epoch 72/100\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 0.0892 - acc: 0.9714\n",
      "Epoch 73/100\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 0.0804 - acc: 0.9619\n",
      "Epoch 74/100\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 0.0819 - acc: 0.9524\n",
      "Epoch 75/100\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 0.0788 - acc: 0.9810\n",
      "Epoch 76/100\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 0.0841 - acc: 0.9524\n",
      "Epoch 77/100\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 0.0753 - acc: 0.9619\n",
      "Epoch 78/100\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 0.0899 - acc: 0.9524\n",
      "Epoch 79/100\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 0.0769 - acc: 0.9714\n",
      "Epoch 80/100\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 0.0802 - acc: 0.9524\n",
      "Epoch 81/100\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 0.0891 - acc: 0.9524\n",
      "Epoch 82/100\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 0.0811 - acc: 0.9714\n",
      "Epoch 83/100\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 0.0804 - acc: 0.9524\n",
      "Epoch 84/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "105/105 [==============================] - 0s 2ms/step - loss: 0.0822 - acc: 0.9524\n",
      "Epoch 85/100\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 0.0770 - acc: 0.9810\n",
      "Epoch 86/100\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 0.0809 - acc: 0.9714\n",
      "Epoch 87/100\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 0.0755 - acc: 0.9810\n",
      "Epoch 88/100\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 0.0831 - acc: 0.9619\n",
      "Epoch 89/100\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 0.0803 - acc: 0.9619\n",
      "Epoch 90/100\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 0.0824 - acc: 0.9619\n",
      "Epoch 91/100\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 0.0770 - acc: 0.9810\n",
      "Epoch 92/100\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 0.0697 - acc: 0.9714\n",
      "Epoch 93/100\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 0.0777 - acc: 0.9714\n",
      "Epoch 94/100\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 0.0804 - acc: 0.9619\n",
      "Epoch 95/100\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 0.0686 - acc: 0.9714\n",
      "Epoch 96/100\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 0.0760 - acc: 0.9619\n",
      "Epoch 97/100\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 0.0907 - acc: 0.9619\n",
      "Epoch 98/100\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 0.0786 - acc: 0.9619\n",
      "Epoch 99/100\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 0.0711 - acc: 0.9619\n",
      "Epoch 100/100\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 0.0756 - acc: 0.9810\n",
      "Training finished \n",
      "\n",
      "Evaluation on test data: loss = 0.094861 accuracy = 97.78% \n",
      "\n",
      "Using model to predict species for features: \n",
      "[[6.1 3.1 5.1 1.1]]\n",
      "\n",
      "Predicted softmax vector is: \n",
      "[[2.0749e-07 9.7903e-01 2.0970e-02]]\n",
      "\n",
      "Predicted species is: \n",
      "Iris-versicolor\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import keras as Keras\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL']='2'\n",
    "\n",
    "# read .csv file and split it into training data and test data\n",
    "def load_data(CSV_FILE_PATH):\n",
    "    IRIS = pd.read_csv(CSV_FILE_PATH)\n",
    "    # This should be our output/label\n",
    "    target_var = 'Name'  \n",
    "    # All the features of the dataset\n",
    "    features = list(IRIS.columns)\n",
    "    features.remove(target_var)\n",
    "    \n",
    "    # find all categories \n",
    "    # Class = {'Iris-setosa' 'Iris-versicolor' 'Iris-virginica'}\n",
    "    Class = IRIS[target_var].unique()\n",
    "    \n",
    "    # create a dictionary for the categories\n",
    "    # let each category be represented by an integer number\n",
    "    # 'Iris-setosa': 0, 'Iris-versicolor': 1, 'Iris-virginica': 2\n",
    "    Class_dict = dict(zip(Class, range(len(Class))))\n",
    "    # print (Class_dict)\n",
    "    \n",
    "    # create a new column where the value is corresponding to its categories\n",
    "    IRIS['category'] = IRIS[target_var].apply(lambda x: Class_dict[x])\n",
    "    # print (IRIS['category'])\n",
    "    \n",
    "    # One-hot Encoding for the categories \n",
    "    lb = LabelBinarizer()\n",
    "    lb.fit(list(Class_dict.values()))\n",
    "    transformed_labels = lb.transform(IRIS['category'])\n",
    "    # print (transformed_labels)\n",
    "    \n",
    "    # create the header for one-hot label\n",
    "    y_labels = []  \n",
    "    for i in range(transformed_labels.shape[1]):\n",
    "        y_labels.append('y' + str(i))\n",
    "        # combine the one-hot columns with the original dataset\n",
    "        IRIS['y' + str(i)] = transformed_labels[:, i]\n",
    "    # print (y_labels)\n",
    "    \n",
    "    # divide the dataset into training data and test data\n",
    "    train_x, test_x, train_y, test_y = train_test_split(IRIS[features], IRIS[y_labels], train_size=0.7, test_size=0.3, random_state=0)\n",
    "    return train_x, test_x, train_y, test_y, Class_dict\n",
    "\n",
    "def main():\n",
    "\n",
    "    # Initialization\n",
    "    print(\"\\nIris dataset using Keras/TensorFlow \")\n",
    "    np.random.seed(4)\n",
    "    tf.set_random_seed(13)\n",
    "\n",
    "    # load data from Iris dataset\n",
    "    print(\"Loading Iris data into memory\")\n",
    "    CSV_FILE_PATH = 'iris.csv'\n",
    "    train_x, test_x, train_y, test_y, Class_dict = load_data(CSV_FILE_PATH)\n",
    "\n",
    "    # define a model\n",
    "    init = Keras.initializers.glorot_uniform(seed=1)\n",
    "    model = Keras.models.Sequential()\n",
    "    # Adds a densely-connected layer with 5 units and relu activation function\n",
    "    model.add(Keras.layers.Dense(units=5, input_dim=4, kernel_initializer=init, activation='relu'))\n",
    "    # Adds a densely-connected layer with 6 units and relu activation function\n",
    "    model.add(Keras.layers.Dense(units=6, kernel_initializer=init, activation='relu'))\n",
    "    # Add a softmax layer with 3 output units:\n",
    "    model.add(Keras.layers.Dense(units=3, kernel_initializer=init, activation='softmax'))\n",
    "    # Configure a model for categorical classification.\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=Keras.optimizers.Adam(), metrics=['accuracy'])\n",
    "\n",
    "    # train the model\n",
    "    b_size = 1\n",
    "    max_epochs = 100\n",
    "    print(\"Starting training \")\n",
    "    h = model.fit(train_x, train_y, batch_size=b_size, epochs=max_epochs, shuffle=True, verbose=1)\n",
    "    print(\"Training finished \\n\")\n",
    "\n",
    "    # evaluate the model\n",
    "    eval = model.evaluate(test_x, test_y, verbose=0)\n",
    "    print(\"Evaluation on test data: loss = %0.6f accuracy = %0.2f%% \\n\" % (eval[0], eval[1] * 100) )\n",
    "\n",
    "    # Prediction\n",
    "    np.set_printoptions(precision=4)\n",
    "    unknown = np.array([[6.1, 3.1, 5.1, 1.1]], dtype=np.float32)\n",
    "    predicted = model.predict(unknown)\n",
    "    print(\"Using model to predict species for features: \")\n",
    "    print(unknown)\n",
    "    print(\"\\nPredicted softmax vector is: \")\n",
    "    print(predicted)\n",
    "    species_dict = {v:k for k,v in Class_dict.items()}\n",
    "    print(\"\\nPredicted species is: \")\n",
    "    print(species_dict[np.argmax(predicted)])\n",
    "\n",
    "main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
